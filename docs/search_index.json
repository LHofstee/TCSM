[
["index.html", "Theory Construction and Statistical Modeling A guide to structural equation modeling in R Course", " Theory Construction and Statistical Modeling A guide to structural equation modeling in R Caspar J. van Lissa¹ ¹Utrecht University, Methodology &amp; Statistics Course In this course you will learn how to translate a social scientific theory into a statistical model, how to analyze your data with these models, and how to interpret and report your results following APA standards. The analyses will be executed using the statistical programming environment R, and in particular using the structural equation modeling package lavaan. "],
["preparing-for-the-course.html", "Chapter 1 Preparing for the course", " Chapter 1 Preparing for the course "],
["installing-software.html", "1.1 Installing software", " 1.1 Installing software Before we start the course, we have to install three things: R, a free program for statistical programming RStudio, a user interface which makes it easier to work with R; overlook our data, packages and output. Several packages, which are ‘add-ons’ for R with functions to do specific analyses. They also include the documentation (help files) that describes how to use them, and sample data. This Chapter shows how you can install RStudio on your computer. We’ll also provide some general information on R, and how you can get help if you get error messages. If you already have RStudio installed on your computer, and if you’re an experienced R user already, all of this might be nothing new for you. You may skip this chapter then. If you have never used R before, this Chapter is essential, as it gives you some input on how R works, and how we can use it for our data analyses. 1.1.1 1. Installing R You will have to install the latest R Version, which is available here. 1.1.2 2. Installing RStudio Download RStudio on the RStudio Website (Link). It’s free! 1.1.3 3. Installing packages As a prerequisite for this guide, you need to have a few essential R packages installed. Open RStudio Inside RStudio, find the window named Console on the bottom left corner of your screen (it might fill the entire left side of the screen). We will now install a few packages using R Code. Here’s an overview of the packages, and why we need them: Package Description lavaan A sophisticated and user-friendly package for structural equation modeling psych A package with convenience functions for screening data, computing scale scores, calculating reliability, etc ggplot2 A flexible and user-friendly package for making graphs 4. To install these packages, we use the install.packages() function in R. One package after another, our code should look like this: install.packages(&quot;lavaan&quot;) install.packages(&quot;psych&quot;) install.packages(&quot;ggplot2&quot;) Don’t forget to put the package names in &quot;&quot;. Otherwise, you will get an error message. 1.1.4 Get started 1.1.5 Starting a new project in Rstudio To keep all your work organized, you should use a project. In Rstudio, click File &gt; New Project &gt; New directory &gt; New project. Type the desired directory name in the dialog (give it a meaningful name, e.g. “My Meta-Analysis”), and use ‘Browse’ if you need to change the directory where you store your projects. Now, in your project, click File &gt; New file &gt; R script. This script file works just like notepad, or the syntax editor in SPSS: You type plain text, but you can run it any time you want. Conduct all of the exercises in this script file. 1.1.6 Code conventions Throughout the guide, a consistent set of conventions is used to refer to code: Functions are in a code font and followed by parentheses, like sum() or mean(). Other R objects (like data or function arguments) are in a code font, without parentheses, like seTE or method.tau. Sometimes, we’ll use the package name followed by two colons, like lavaan::sem(). This is valid R code and will run. The lavaan:: part indicates that the function sem() comes from the package lavaan. 1.1.7 Getting Help As you start to apply the techniques described in this guide to your data you will soon find questions that the guide does not answer. This section describes a few tips on how to get help. Every function in R has documentation (a help file). To see it, select the name of the function and press F1, or run the command ? followed by the name of the function, e.g.: ?aov. Andy Field, the book used for our undergraduate statistics courses (Field, Miles, and Field 2012), is also available for R. Many basic analyses are explained for R in this book. If you get stuck, start with Google. Typically, adding “R” to a search is enough to restrict it to relevant results, e.g.: “exploratory factor analysis R”. Google is particularly useful for error messages. If you get an error message and you have no idea what it means, try googling it. Chances are that someone else has been confused by it in the past, and there will be help somewhere on the web. (If the error message isn’t in English, run Sys.setenv(LANGUAGE = &quot;en&quot;) and re-run the code; you’re more likely to find help for English error messages.) If Google doesn’t help, try stackoverflow. Start by spending a little time searching for an existing answer; including [R] restricts your search to questions and answers that use R. Lastly, if you stumble upon an error (or typos!) in this guide’s text or R syntax, feel free to contact Caspar van Lissa at c.j.vanlissa@uu.nl. References "],
["getting-your-data-into-r.html", "Chapter 2 Getting your data into R", " Chapter 2 Getting your data into R This optional chapter will tell you about how you can import data in RStudio. We will also show you a few commands to manipulate data directly in R. "],
["using-r-projects.html", "2.1 Using R projects", " 2.1 Using R projects One advantage of using an R project is that the project directory is automatically set as the working directory. Just copy your data file to the folder that contains the “.Rproj” file, and you will be able to load files by name. "],
["importing-excel-files.html", "2.2 Importing Excel Files", " 2.2 Importing Excel Files One way to get Excel files directly into R is by using the XLConnect package. Install the package, and try using the readWorksheetFromFile() function to load the data, and assign it to an object called df: # Run this only once, to download and install the package: install.packages(&quot;XLConnect&quot;) # Load the package: library(XLConnect) # Read an Excel file into &#39;df&#39;: df &lt;- readWorksheetFromFile(&quot;your_file.xlsx&quot;, sheet = 1) 2.2.1 Inspect the data R does not work with a single spreadsheet (SPSS or Excel). Instead, it can keep many objects in memory. The object df is a data.frame; an object that behaves similar to a spreadsheet. To see a description of the object, look at the Environment tab in the top right of Rstudio, and click the arrow next to df. As you can see, the on the top-right pane Environment, your file is now listed as a data set in your RStudio environment. You can make a quick copy of this data set by assigning the df object to a new object. This way, you can edit one, and leave the other unchanged. Assign the object df to a new object called df_backup: df_backup &lt;- df You can also have a look at the contents of df by clicking the object in the Environment panel, or running the command head(df). "],
["importing-spss-files.html", "2.3 Importing SPSS Files", " 2.3 Importing SPSS Files SPSS files can be loaded using the foreign package. All SPSS files for this course are available on Blackboard. # Install the package, run this only once install.packages(&quot;foreign&quot;) # Load the `foreign` library library(foreign) # Read the SPSS data df &lt;- read.spss(&quot;sesam2.sav&quot;, to.data.frame = TRUE) "],
["data-manipulation-optional.html", "2.4 Data manipulation (optional)", " 2.4 Data manipulation (optional) Now that we have the Meta-Analysis data in RStudio, let’s do a few manipulations with the data. These functions might come in handy when were conducting analyses later on. Going back to the output of the str() function, we see that this also gives us details on the type of column data we have stored in our data. There a different abbreviations signifying different types of data. Abbreviation Type Description num Numerical This is all data stored as numbers (e.g. 1.02) chr Character This is all data stored as words log Logical These are variables which are binary, meaning that they signify that a condition is either TRUE or FALSE factor Factor Factors are stored as numbers, with each number signifying a different level of a variable. A possible factor of a variable might be 1 = low, 2 = medium, 3 = high 2.4.1 Converting to factors Let’s look at the variable df$VIEWCAT. This is a categorical variable, coded as a numerical one. We can have a look at this variable by typing the name of our dataset, then adding the selector $ and then adding the variable we want to have a look at. This variable is currently a numeric vector. We want it to be a factor: That’s a categorical variable. To convert this to a factor variable now, we use the factor() function. df$VIEWCAT &lt;- factor(df$VIEWCAT) We now see that the variable has been converted to a factor with the levels “1”, “2”, “3”, and “4”. We can assign different value labels as follows: df$VIEWCAT &lt;- factor(df$VIEWCAT, labels = c(&quot;Rarely&quot;, &quot;Sometimes&quot;, &quot;Regularly&quot;, &quot;Often&quot;)) 2.4.2 Selecting specific cases It may often come in handy to select certain cases for further analyses, or to exclude some studies in further analyses (e.g., if they are outliers). To do this, we can use the [] operator to index our data. Let’s say we want to get only the first 5 cases. We can select them like so: df[1:5, ] Or let’s say we only want the children younger than 36 months in the dataset. In this case, we can use boolean indexing: We create a TRUE / FALSE statement, and select the cases that are TRUE: df[df$AGE &lt; 36, ] Note that this approach can be used for any other type of data and variable. We can also use it to e.g., only select studies where VIEWCAT was equal to “Often” “typical”: df[df$VIEWCAT == &quot;Often&quot;, ] 2.4.3 Changing cell values Sometimes, even when preparing your data in EXCEL, you might want to change values in RStudio once you have imported your data. To do this, we have to select a cell in our data frame in RStudio. This can be done by adding [x,y] to our dataset name, where x signifies the number of the row we want to select, and y signifies the number of the column. To see how this works, let’s select a variable using this command first: df[8,1] ## [1] 8 We now see the 8th study in our dataframe, and the value of this study for Column 1 (participant ID) is displayed. Let’s say we had a typo in this name and want to have it changed. In this case, we have to give this exact cell a new value. df[8,1] &lt;- 1001 Let’s check if the value has changed. df[8,1] ## [1] 1001 You can also use this function to change any other type of data, including numericals and logicals. Only for characters, you have to put the values you want to insert in &quot;&quot;. "],
["week-1-home.html", "Chapter 3 Week 1 - Home", " Chapter 3 Week 1 - Home Open the data file LifeSat.sav. library(foreign) data &lt;- read.spss(&quot;LifeSat.sav&quot;, to.data.frame = TRUE) 3.0.1 Question 1.a Make a descriptives table for the variables: LifSat, educ, ChildSup, SpouSup, and age. What is the average age in the sample? And the range (youngest and oldest child)? Hint: Use library(psych); describe(); [] Click for explanation The package psych contains many functions for exploring data. Install and load the package, then use the describe() function to describe the data: library(psych) describe(data[, c(&quot;LifSat&quot;, &quot;educ&quot;, &quot;ChildSup&quot;, &quot;SpouSup&quot;, &quot;age&quot;)]) &lt;&gt; 3.0.2 Question 1.b Perform a simple regression with LifeSat as the dependent variable and educ as the independent variable. Click for explanation results &lt;- lm(LifSat ~ educ, data) summary(results) ## ## Call: ## lm(formula = LifSat ~ educ, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -43.781 -11.866 2.018 12.418 43.018 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 35.184 7.874 4.469 2.15e-05 *** ## educ 3.466 1.173 2.956 0.00392 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 17.64 on 96 degrees of freedom ## Multiple R-squared: 0.08344, Adjusted R-squared: 0.0739 ## F-statistic: 8.74 on 1 and 96 DF, p-value: 0.003918 &lt;&gt; 3.0.3 Question 1.c. Do the same with age as the independent variable. Click for explanation results &lt;- lm(LifSat ~ age, data) summary(results) ## ## Call: ## lm(formula = LifSat ~ age, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -35.321 -14.184 3.192 13.593 40.626 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 200.2302 52.1385 3.840 0.00022 *** ## age -2.0265 0.7417 -2.732 0.00749 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 17.75 on 96 degrees of freedom ## Multiple R-squared: 0.07215, Adjusted R-squared: 0.06249 ## F-statistic: 7.465 on 1 and 96 DF, p-value: 0.007487 &lt;&gt; 3.0.4 Question 1.d. Again with ChildSup as the independent variable. Click for explanation results &lt;- lm(LifSat ~ ChildSup, data) summary(results) ## ## Call: ## lm(formula = LifSat ~ ChildSup, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -37.32 -12.14 0.66 12.41 44.68 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 37.559 8.342 4.502 1.89e-05 *** ## ChildSup 2.960 1.188 2.492 0.0144 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 17.86 on 96 degrees of freedom ## Multiple R-squared: 0.06076, Adjusted R-squared: 0.05098 ## F-statistic: 6.211 on 1 and 96 DF, p-value: 0.01441 &lt;&gt; 3.0.5 Question 1.e. Perform a multiple regression with LifeSat as the dependent variable and educ, age and ChildSup as the independent variables. Click for explanation results &lt;- lm(LifSat ~ educ + age + ChildSup, data) summary(results) ## ## Call: ## lm(formula = LifSat ~ educ + age + ChildSup, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -32.98 -12.56 2.68 11.03 41.91 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 134.9801 53.2798 2.533 0.0130 * ## educ 2.8171 1.1436 2.463 0.0156 * ## age -1.5952 0.7188 -2.219 0.0289 * ## ChildSup 2.4092 1.1361 2.121 0.0366 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 16.92 on 94 degrees of freedom ## Multiple R-squared: 0.1741, Adjusted R-squared: 0.1477 ## F-statistic: 6.603 on 3 and 94 DF, p-value: 0.0004254 &lt;&gt; 3.0.6 Question 1.f. Compare the results under 1.e with those obtained under 1.b-1.d. What do you notice when you compare the regression parameter for each of the three predictors in the multiple regression with the corresponding regression parameters obtained in the simple regressions? "],
["week-1-class.html", "Chapter 4 Week 1 - Class", " Chapter 4 Week 1 - Class During the practical you will work on some exercises about ANOVA and ANCOVA using regression and path modeling. Note that ANOVA and ANCOVA are special cases of regression, as discussed during MTS3 or a similar course. How to perform an ANOVA/ANCOVA as a regression analysis is prerequisite knowledge. This practical we will work on these topics (ANOVA, ANCOVA, regression and how they are related). If you need to refresh your knowledge you could use the internet to find information or you could look it up in a book on statistics, for example Field, Miles, and Field (2012) (The chapters on ANOVA, Factorial ANOVA, and ANCOVA (11.6)). We start with two exercises in which you have to explore your data and perform a regression analysis, ANOVA and an ANCOVA. You will also practice with performing an ANCOVA as a regression analysis in exercise 3 today. References "],
["loading-data.html", "4.1 Loading data", " 4.1 Loading data Open the file Sesam.sav: # Library for reading SPSS files: library(foreign) # Load the data and put them in the object called &quot;data&quot; data &lt;- read.spss(&quot;sesam.sav&quot;, to.data.frame = TRUE, use.value.labels = FALSE) This file is part of a larger dataset that evaluates the impact of the first year of the Sesame Street television series. Sesame Street is mainly concerned with teaching preschool related skills to children in the 3-5 year age range. The following variables will be used in this exercise: age measured in months prelet knowledge of letters before watching Sesame Street (range 0-58) prenumb knowledge of numbers before watching Sesame Street (range 0-54) prerelat knowledge of relations before watching Sesame Street (range 0-17) peabody vocabulary maturity before watching Sesame Street (range 20-120) postnumb knowledge of numbers after a year of Sesame Street (range 0-54) "],
["section-1.html", "4.2 Section 1", " 4.2 Section 1 4.2.1 Question 1.a What is the level of measurement of each of the variables? Click for explanation In the ‘Environment’ panel in the top right corner of the screen, click the arrow in the next to the object called ‘data’. Alternatively, run the rode: head(data). 4.2.2 Question 1.b What is the average age in the sample? And the range (youngest and oldest child)? Hint: Use install.packages(“psych”); library(psych); describe() Click for explanation The package psych contains many functions for exploring data. Install and load the package, then use the describe() function to describe the data: install.packages(&quot;psych&quot;) library(psych) describe(data) &lt;&gt; 4.2.3 Question 1.c What is the average gain in knowledge of numbers? Provide both the mean and the standard deviation. Hint: Use the &lt;- operator to assign to a new variable in data. Functions mean() and sd(). Click for explanation Create a new variable that represents the difference between pre- and post-test scores: data$dif &lt;- data$postnumb - data$prenumb There are specialized functions to obtain the mean and sd: mean(data$dif) ## [1] 9.158333 sd(data$dif) ## [1] 9.682401 &lt;&gt; 4.2.4 Question 1.d Choose an appropriate graph to present the gain scores. What did you choose and why? Hint: Several useful plotting functions for univariate distributions are: hist(); plot(density()); boxplot() Click for explanation plot(density(data$dif)) &lt;&gt; 4.2.5 Question 1.e Can you think of a graph based on two variables (i.e. the difference variable and another variable) that is informative? What is it and how is it informative? Hint: A useful plotting function for a bivariate distribution is the scatterplot: plot(data$x, data$y) Click for explanation plot(data$prenumb, data$postnumb) &lt;&gt; 4.2.6 Question 1.f Which of the variables age, prelet, prenumb, prerelat and peabody are significantly related to postnumb? Use Pearson’s correlations (cor()). You don’t need to check assumptions. Click for explanation Hint: The function cor() provides Pearson’s correlations. Select variables by name from a data.frame object (like data*) using the following syntax: cor(data[, c(&quot;age&quot;, &quot;prelet&quot;, &quot;prenumb&quot;, &quot;prerelat&quot;, &quot;peabody&quot;, &quot;postnumb&quot;)]) ## age prelet prenumb prerelat peabody postnumb ## age 1.0000000 0.3278806 0.4328262 0.4422787 0.2939283 0.3410578 ## prelet 0.3278806 1.0000000 0.7173013 0.4712200 0.3958917 0.5038464 ## prenumb 0.4328262 0.7173013 1.0000000 0.7175289 0.6141059 0.6755051 ## prerelat 0.4422787 0.4712200 0.7175289 1.0000000 0.5551258 0.5433818 ## peabody 0.2939283 0.3958917 0.6141059 0.5551258 1.0000000 0.5201280 ## postnumb 0.3410578 0.5038464 0.6755051 0.5433818 0.5201280 1.0000000 The use of data[,] follows the conventions of matrix indexation: You can select rows like this, data[i, ], and columns like this, data[ ,j], where i are the rows and j are the columns you want to select. &lt;&gt; 4.2.7 Question 1.g Can age and prenumb be used to predict postnumb? If so, discuss the substantial importance of the model and the significance and substantial importance of the separate predictors. Hint: The function lm() (short for linear model) conducts linear regression. The functions summary() provides relevant summary statistics for the model. It can be helpful to store the results of your analysis in an object, too. Click for explanation results &lt;- lm(formula = postnumb ~ age + prenumb, data = data) summary(results) ## ## Call: ## lm(formula = postnumb ~ age + prenumb, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -38.130 -6.456 -0.456 5.435 22.568 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.4242 5.1854 1.432 0.154 ## age 0.1225 0.1084 1.131 0.259 ## prenumb 0.7809 0.0637 12.259 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.486 on 237 degrees of freedom ## Multiple R-squared: 0.4592, Adjusted R-squared: 0.4547 ## F-statistic: 100.6 on 2 and 237 DF, p-value: &lt; 2.2e-16 &lt;&gt; 4.2.8 Question 1.h Provide the null hypotheses and the alternative hypotheses of the model in 1.g. Click for explanation The null-hypotheses of the model pertain to the variance explained: \\(P^2\\) (that’s Greek letter rho, for the population value of \\(R^2\\)). \\(H_0: P^2 = 0\\) \\(H_a: P^2 &gt; 0\\) &lt;&gt; 4.2.9 Question 1.i Consider the path model below. How many regression coefficients are estimated in this model? And how many variances? And how many covariances? How many degrees of freedom does this model have? (\\(df = N_{obs} – N_{par}\\), see slides Lecture 1). 4.2.10 Question 1.j Consider a multiple regression analysis with three continuous independent variables, tests in language, history and logic, and one continuous dependent variable, a score on a math test. We want to know whether the various tests can predict the math score. Sketch a path model for this analysis (there are examples in the lecture slides of week 1). How many regression parameters are there? How many variances could you estimate? How many covariances could you estimate? How many degrees of freedom does this model have? "],
["section-2.html", "4.3 Section 2", " 4.3 Section 2 Open the file drivers.sav. 4.3.1 Research question 1 (ANOVA): Does talking on the phone interfere with people’s driving skills? IV: condition hand-held phone hands-free phone control DV: reaction time in milliseconds in a driver simulation test. # Load the data and put them in the object called &quot;data&quot; data &lt;- read.spss(&quot;drivers.sav&quot;, to.data.frame = TRUE) 4.3.2 Question 2.a Perform the ANOVA. Hint: The function aov() is an alternate interface to the linear model (lm), which reports results in line with the convention of ANOVA analyses. Click for explanation results &lt;- aov(formula = RT ~ condition, data = data) summary(results) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## condition 2 103909 51954 3.072 0.0541 . ## Residuals 57 964082 16914 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 &lt;&gt; 4.3.3 Question 2.b What are the assumptions you need to check? Click for explanation We can check several assumptions: Presence of outliers Normality of residuals Homogeneity of residuals Let’s deal with them in order. 4.3.3.1 Presence of outliers: In Y-space We can check the range of the standardized (scale()) residuals for outliers in Y-space. The residuals are inside of the results object, so we can just extract them, standardize them, and get the range: range(scale(results$residuals)) ## [1] -2.483778 1.904491 4.3.3.2 Normality of residuals We can check the normality of residuals using a QQplot. qqnorm(results$residuals) qqline(results$residuals) There appears to be some mild deviation from normality at the extremes. You can also test for normality with the shapiro.test(x) function: shapiro.test(results$residuals) ## ## Shapiro-Wilk normality test ## ## data: results$residuals ## W = 0.98367, p-value = 0.6013 4.3.3.3 Homogeneity of Variances The bartlett.test() function provides a parametric K-sample test of the equality of variances: bartlett.test(formula = RT~condition, data = data) ## ## Bartlett test of homogeneity of variances ## ## data: RT by condition ## Bartlett&#39;s K-squared = 2.7203, df = 2, p-value = 0.2566 It can also be nice to use a paneled boxplot to visualize the distributions. For this, we will use the package ggplot2: install.packages(&quot;ggplot2&quot;) library(ggplot2) ggplot(data, aes(y = RT, group = condition)) + geom_boxplot() + theme_bw() &lt;&gt; 4.3.4 Question 2.c Explain for each of the assumptions why they are important to check. 4.3.5 Question 2.d What are your conclusions regarding the assumption checks? Klik voor meer uitleg There are no outliers in X-space, no evidence for (severe) deviations from normality of residuals, and no evidence for (severe) heteroscedasticity. 4.3.6 Question 2.e Answer the research question. Hint: Use summary() and pairwise.t.test(). Click for explanation We can examine the overall F-test, which is significant: summary(results) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## condition 2 103909 51954 3.072 0.0541 . ## Residuals 57 964082 16914 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Post-hoc tests with Bonferroni correction can be obtained using pairwise.t.test(). We notice that none of these comparisons are significant. However, the research question was Does talking on the phone interfere with people’s driving skills? There are two conditions for talking on the phone. We could thus test a planned contrast of these two conditions against the control condition, instead of all possible post-hoc tests: The standard contrasts are dummy coded: contrasts(data$condition) ## hands-free control ## hand-held 0 0 ## hands-free 1 0 ## control 0 1 We can replace these with planned contrasts for “phone” vs control, and hand-held vs hands-free: contrasts(data$condition) &lt;- cbind(phoneVcontrol = c(-1, -1, 2), handVfree = c(-1, 1, 0)) results &lt;- aov(RT ~ condition, data) # Ask for the lm summary, which gives you t-tests for the planned contrasts: summary.lm(results) ## ## Call: ## aov(formula = RT ~ condition, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -317.50 -71.25 2.98 89.55 243.45 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 608.60 16.79 36.248 &lt;2e-16 *** ## conditionphoneVcontrol -27.42 11.87 -2.310 0.0245 * ## conditionhandVfree -18.47 20.56 -0.898 0.3727 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 130.1 on 57 degrees of freedom ## Multiple R-squared: 0.09729, Adjusted R-squared: 0.06562 ## F-statistic: 3.072 on 2 and 57 DF, p-value: 0.05408 &lt;&gt; 4.3.7 Research question 2 (ANCOVA): Are there differences in reaction time between the conditions when controlling for age? 4.3.8 Question 2.f Perform the ANCOVA. Click for explanation results &lt;- aov(formula = RT ~ condition + age, data = data) summary(results) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## condition 2 103909 51954 4.52 0.0151 * ## age 1 320454 320454 27.88 2.18e-06 *** ## Residuals 56 643627 11493 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 &lt;&gt; 4.3.9 Question 2.g What are the assumptions you need to check? Click for explanation Assumptions for ANCOVA are the same as for ANOVA (no outliers, normality of residuals, homoscedasticity). ANCOVA has the following additional assumptions: Homogeneity of regression slopes for the covariate (no interaction between factor variable and covariate) The covariate is independent of the treatment effects &lt;&gt; 4.3.10 Question 2.h Explain for each of the assumptions why they are important to check. 4.3.11 Question 2.i Check the assumptions of ANCOVA. Click for explanation 4.3.11.1 Homogeneity of regression slopes Compare a model with only main effects to one with an interaction between age and condition: results_age &lt;- aov(RT ~ condition + age, data) results_age_int &lt;- aov(RT ~ condition * age, data) anova(results_age, results_age_int) The interaction is NOT significant; no evidence for violation of the assumption. 4.3.11.2 The covariate is independent of the treatment effects results_indep &lt;- aov(age ~ condition, data) summary(results_indep) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## condition 2 137 68.55 0.659 0.521 ## Residuals 57 5926 103.97 The covariate is not significantly related to treatment effect. 4.3.11.3 Outliers in X-space In addition to the aforementioned outliers in Y-space, we can now test for (multivariate) outliers in X-space using Mahalanobis’ distance. In this case, we only have one continuous covariate. The function requires a matrix of data, a vector of means for centering the data, and a covariance matrix. In the syntax below, we use drop = FALSE when extracting a single column from the data, in order to make sure that the data will still be in matrix format when we extract only one column. This is necessary for the underlying matrix algebra. We have to take the sqrt() because the function mahalanobis() returns the squared Mahalanobis’ distances. mahal &lt;- sqrt(mahalanobis(data[ , &quot;age&quot;, drop = FALSE], center = mean(data$age), cov = cov(data[, &quot;age&quot;, drop = FALSE]))) range(mahal) ## [1] 0.01972835 1.79527971 &lt;&gt; 4.3.12 Question 2.j Answer the research question. "],
["section-3.html", "4.4 Section 3", " 4.4 Section 3 Open the file Sesam2.sav. # Load the data and put them in the object called &quot;data&quot; data &lt;- read.spss(&quot;sesam2.sav&quot;, to.data.frame = TRUE) Use postnumb as the dependent variable in all the following analyses. 4.4.1 Question 3.a Viewcat is a factor variable, but is not coded as such in the data. Turn it into a factor. Afterwards, make sure that viewcat=1 is the reference group in the contrasts, i.e., the group that is identified by zero scores on all the associated dummy variables. Hint: Use &lt;- factor() and contrasts(). Click for explanation data$VIEWCAT &lt;- factor(data$VIEWCAT) contrasts(data$VIEWCAT) ## 2 3 4 ## 1 0 0 0 ## 2 1 0 0 ## 3 0 1 0 ## 4 0 0 1 &lt;&gt; 4.4.2 Question 3.b Perform a multiple regression analysis with just the viewcat dummies as predictors. Click for explanation results &lt;- lm(POSTNUMB ~ VIEWCAT, data) summary(results) ## ## Call: ## lm(formula = POSTNUMB ~ VIEWCAT, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -25.474 -7.942 0.240 8.526 25.240 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 18.760 2.316 8.102 8.95e-14 *** ## VIEWCAT2 9.331 2.900 3.218 0.00154 ** ## VIEWCAT3 14.714 2.777 5.298 3.49e-07 *** ## VIEWCAT4 18.032 2.809 6.419 1.24e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 11.58 on 175 degrees of freedom ## Multiple R-squared: 0.2102, Adjusted R-squared: 0.1967 ## F-statistic: 15.53 on 3 and 175 DF, p-value: 5.337e-09 &lt;&gt; 4.4.3 Question 3.c What do the regression coefficients represent? How can you determine the average postnumb score for each of the viewcat categories, based on the regression parameters? 4.4.4 Question 3.d Make a coloured scatter plot with age on the x-axis and postnumb on the y-axis. Colour the dots according to the their ‘viewcat’ category. Double click the plot and add a linear fit line for each of the four viewcat scores separately. How do you interpret the differences in slopes of these four fit lines? Hint: Use ggplot(); geom_point(); geom_smooth() Click for explanation We will use ggplot again: ggplot(data, aes(x = AGE, y = POSTNUMB, colour = VIEWCAT)) + geom_point() + # For scatterplot geom_smooth(method = &quot;lm&quot;, se = FALSE) + # For regression lines theme_bw() # For a pretty theme &lt;&gt; 4.4.5 Question 3.e Add an interaction between age and viewcat. Hint: An interaction is created by multiplying two variables. You can multiply with * in the formula of lm(). Click for explanation results_interaction &lt;- lm(POSTNUMB ~ VIEWCAT*AGE, data) summary(results_interaction) ## ## Call: ## lm(formula = POSTNUMB ~ VIEWCAT * AGE, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -23.8371 -8.2387 0.6158 8.7988 22.5611 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -18.7211 15.5883 -1.201 0.2314 ## VIEWCAT2 9.9741 20.6227 0.484 0.6293 ## VIEWCAT3 23.5825 19.3591 1.218 0.2248 ## VIEWCAT4 34.3969 19.3600 1.777 0.0774 . ## AGE 0.7466 0.3074 2.429 0.0162 * ## VIEWCAT2:AGE -0.0175 0.4060 -0.043 0.9657 ## VIEWCAT3:AGE -0.1930 0.3782 -0.510 0.6104 ## VIEWCAT4:AGE -0.3416 0.3770 -0.906 0.3663 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.99 on 171 degrees of freedom ## Multiple R-squared: 0.3046, Adjusted R-squared: 0.2762 ## F-statistic: 10.7 on 7 and 171 DF, p-value: 3.79e-11 &lt;&gt; 4.4.6 Question 3.f Perform a sequential multiple regression. Include age and viewcat as the predictors in the first analysis. Add the interaction term in the second analysis. Make sure to obtain information about the change in R-square! Hint: Use anova() to compare two regression models. Click for explanation results_main &lt;- lm(POSTNUMB ~ VIEWCAT + AGE, data) anova(results_main, results_interaction) &lt;&gt; 4.4.7 Question 3.g Sketch path models of both steps of the regression analysis (on paper). 4.4.8 Question 3.h Write down the regression equations of both steps of the sequential analysis. Click for explanation \\(Postnumb_i = b_0 + b_1D_{view2i} + b_2D_{view3i} + b_3D_{view4i} + b_4Age_i + \\epsilon_i\\) \\(Postnumb_i = b_0 + b_1D_{view2i} + b_2D_{view3i} + b_3D_{view4i} + b_4Age_i + b_5D_{view2i}Age_i + b_6D_{view3i}Age_i + b_7D_{view4i}Age_i + \\epsilon_i\\) &lt;&gt; 4.4.9 Question 3.i Write down the null hypothesis that is tested to determine whether there is an interaction between age and viewcat. Click for explanation \\(H_0: \\DeltaP^2 = 0\\) &lt;&gt; 4.4.10 Question 3.j Indicate for each parameter in the second regression model what it means. Also write down the regression equation for each of the four categories of viewcat separately. 4.4.11 Question 3.k What do you conclude about the interaction between age and viewcat? 4.4.12 Question 3.l Note that you can also look at this problem as an ANCOVA. What are the research question and null hypothesis in this case? Click for explanation RQ: Is there a significant difference between the marginal means of postnumb by viewcat, after controlling for age? &lt;&gt; 4.4.13 Question 3.m Perform this analysis as an ANCOVA. Hint: Add -1 to a formula to drop the intercept. Click for explanation To drop the intercept from the analysis, and estimate the marginal means for all viewcat categories, we can add -1 (minus the intercept) to the formula: results_ancov &lt;- aov(POSTNUMB~AGE+VIEWCAT-1, data) &lt;&gt; Examine the parameter estimates of the ANCOVA. What do the parameter estimates represent? Click for explanation We use summary.lm() again to obtain the parameter estimates: summary.lm(results_ancov) ## ## Call: ## aov(formula = POSTNUMB ~ AGE + VIEWCAT - 1, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -23.680 -8.003 -0.070 8.464 22.635 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## AGE 0.5750 0.1221 4.708 5.08e-06 *** ## VIEWCAT1 -10.1056 6.5091 -1.553 0.122 ## VIEWCAT2 -0.9603 6.3865 -0.150 0.881 ## VIEWCAT3 3.7546 6.4760 0.580 0.563 ## VIEWCAT4 6.8159 6.5414 1.042 0.299 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.94 on 174 degrees of freedom ## Multiple R-squared: 0.8973, Adjusted R-squared: 0.8943 ## F-statistic: 304 on 5 and 174 DF, p-value: &lt; 2.2e-16 &lt;&gt; "],
["week-2-home.html", "Chapter 5 Week 2 - Home", " Chapter 5 Week 2 - Home This exercise is based on Kestilä, Elina (2006) Is There Demand for Radical Right Populism in the Finnish Electorate? Scandinavian Political Studies 29(3),169-191 You have read and answered questions about the article in the reading questions. In this exercise, as well as in the second class practical, we will analyze these data ourselves. The data for this practical stem from the first round of the European Social Survey (ESS). This is a repeated cross-sectional survey across 32 European countries. The first round was held in 2002, and since then, subsequent rounds of data-collection are held bi- anually. More info, as well as access to all data -&gt; www.europeansocialsurvey.org. The raw, first round data can also be found on blackboard. The file is called ESSround1- a.sav. This file contains data for all respondents, but only those variables are included that you will need in this exercise. 5.0.1 Question 1.a Download the file, and import it in R. Inspect the file (no. of cases and no. of variables) to see if the file opened well. library(foreign) data &lt;- read.spss(&quot;ESSround1-a.sav&quot;, to.data.frame = TRUE) For a description of all variables in the dataset, click here! Variable Description name Title of dataset essround ESS round edition Edition proddate Production date cntry Country idno Respondent’s identification number trstlgl Trust in the legal system trstplc Trust in the police trstun Trust in the United Nations trstep Trust in the European Parliament trstprl Trust in country’s parliament stfhlth State of health services in country nowadays stfedu State of education in country nowadays stfeco How satisfied with present state of economy in country stfgov How satisfied with the national government stfdem How satisfied with the way democracy works in country pltinvt Politicians interested in votes rather than peoples opinions pltcare Politicians in general care what people like respondent think trstplt Trust in politicians imsmetn Allow many/few immigrants of same race/ethnic group as majority imdfetn Allow many/few immigrants of different race/ethnic group from majority eimrcnt Allow many/few immigrants from richer countries in Europe eimpcnt Allow many/few immigrants from poorer countries in Europe imrcntr Allow many/few immigrants from richer countries outside Europe impcntr Allow many/few immigrants from poorer countries outside Europe qfimchr Qualification for immigration: christian background qfimwht Qualification for immigration: be white imwgdwn Average wages/salaries generally brought down by immigrants imhecop Immigrants harm economic prospects of the poor more than the rich imtcjob Immigrants take jobs away in country or create new jobs imbleco Taxes and services: immigrants take out more than they put in or less imbgeco Immigration bad or good for country’s economy imueclt Country’s cultural life undermined or enriched by immigrants imwbcnt Immigrants make country worse or better place to live imwbcrm Immigrants make country’s crime problems worse or better imrsprc Richer countries should be responsible for accepting people from poorer countries pplstrd Better for a country if almost everyone share customs and traditions vrtrlg Better for a country if a variety of different religions shrrfg Country has more than its fair share of people applying refugee status rfgawrk People applying refugee status allowed to work while cases considered gvrfgap Government should be generous judging applications for refugee status rfgfrpc Most refugee applicants not in real fear of persecution own countries rfggvfn Financial support to refugee applicants while cases considered rfgbfml Granted refugees should be entitled to bring close family members gndr Gender yrbrn Year of birth edulvl Highest level of education eduyrs Years of full-time education completed polintr How interested in politics lrscale Placement on left right scale 5.0.2 Question 1.b The ESS-file contains much more information than we need to re-analyze the paper by Kestilä. We need to reduce the number of cases, in order to make the file more manageable, and make sure our results pertain to our target population. Kestilä only uses data from ten countries: c(&quot;Austria&quot;, &quot;Belgium&quot;, &quot;Denmark&quot;, &quot;Finland&quot;, &quot;France&quot;, &quot;Germany&quot;, &quot;Italy&quot;, &quot;Netherlands&quot;, &quot;Norway&quot;, &quot;Sweden&quot;). Select data from these countries by means of boolean indexing, using the %in% function. Hint: Use []; %in% Click for explanation df &lt;- data[data$cntry %in% c(&quot;Austria&quot;, &quot;Belgium&quot;, &quot;Denmark&quot;, &quot;Finland&quot;, &quot;France&quot;, &quot;Germany&quot;, &quot;Italy&quot;, &quot;Netherlands&quot;, &quot;Norway&quot;, &quot;Sweden&quot;), ] 5.0.3 Question 1.c Inspect the data file again to see whether step 1b went ok. 5.0.4 Question 1.d Before we can start the analyses, we first need to screen the data. What are the things we need to watch for? (think about your earlier statistics-courses)? Click for explanation This question is open to interpretation. One thing you might notice is that all variables the authors used are currently coded as factor variables (e.g., “Factor w/ 11 levels”): levels(df$trstlgl) ## [1] &quot;No trust at all&quot; &quot;1&quot; &quot;2&quot; ## [4] &quot;3&quot; &quot;4 &quot; &quot;5&quot; ## [7] &quot;6&quot; &quot;7&quot; &quot;8&quot; ## [10] &quot;9&quot; &quot;Complete trust&quot; In keeping with conventions, we could treat ordinal Likert scales with &gt;5 levels as continuous. We can either re-code the data, or prevent read.spss() from coding these variables as factors when it reads the data. Here is code for both approaches. 5.0.4.1 Re-coding factors to numeric Data.frames like df are (secretly) lists, where each column is an element of the list. That means we can index a data.frame as a list, not just as a matrix. We can also use the incredibly powerful lapply() function, short for list apply, which takes each list element (column), and applies a function to it. In this case, that function is as.numeric(), which turns factors into numbers: df[7:44] &lt;- lapply(df[7:44], as.numeric) table(df[,7]) ## ## 1 2 3 4 5 6 7 8 9 10 11 ## 716 434 908 1456 1634 3196 2454 3066 3266 1517 733 5.0.4.2 Reading data without coding factors An alternative solution is to stop the function read.spss() from using value labels to code variables as factors. However, we’re not going to use this right now, so the information below is merely illustrative: # The option use.value.labels = FALSE stops the function from coding factors: data &lt;- read.spss(&quot;ESSround1-a.sav&quot;, to.data.frame = TRUE, use.value.labels = FALSE) # Then, re-select the subset of data. The countries are now also unlabeled, so # we select them by number: df &lt;- data[data$cntry %in% c(21,18,17,15,9,8,6,5,2,1), ] 5.0.5 Question 1.e Screen all your variables univariately (one by one) by making frequency tables for every variable. Again, unsure how to do this? Review Field. Pay particular attention to the measurement scales of each variable. Hint: Use table(); lapply(); psych::describe() Click for explanation We can use the lapply() function from the previous question to get frequency tables for all relevant variables: lapply(df[7:44], table) ## $trstlgl ## ## 1 2 3 4 5 6 7 8 9 10 11 ## 716 434 908 1456 1634 3196 2454 3066 3266 1517 733 ## ## $trstplc ## ## 1 2 3 4 5 6 7 8 9 10 11 ## 346 255 494 855 1080 2462 2326 3637 4318 2348 1439 This is probably a bit too much detail, so we can just use the psych::describe() fuction again: psych::describe(df[7:44]) 5.0.6 Question 1.f Are there any incorrectly coded missing value labels, or other inexplicable values? 5.0.7 Question 1.g The first step in re-analyzing data is replicating the results from the paper by Kestilä. Run a Principal Component Analysis using psych::principal(), and choose the exact same specification as Kestilä concerning estimation method, rotation etc. Do two analyses: one for trust in politics, and one for attitudes towards immigration. Hint: Use psych::principal() Click for explanation There’s a convenient function for PCA in the psych package, although base R also has the function princomp(). 5.0.7.1 Trust in politics Kestilä extracted three components, with VARIMAX rotation. When we print the results, we can hide all factor loadings smaller than the smallest one in their table, to make it easier to read: library(psych) pca_trust &lt;- principal(df[, 7:19], nfactors = 3, rotate = &quot;varimax&quot;) print(pca_trust, cut = .3, digits =3) ## Principal Components Analysis ## Call: principal(r = df[, 7:19], nfactors = 3, rotate = &quot;varimax&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## RC3 RC2 RC1 h2 u2 com ## trstlgl 0.779 0.669 0.331 1.21 ## trstplc 0.761 0.633 0.367 1.18 ## trstun 0.675 0.556 0.444 1.44 ## trstep 0.651 0.332 0.549 0.451 1.57 ## trstprl 0.569 0.489 0.650 0.350 2.49 ## stfhlth 0.745 0.567 0.433 1.04 ## stfedu 0.750 0.603 0.397 1.14 ## stfeco 0.711 0.300 0.616 0.384 1.44 ## stfgov 0.634 0.377 0.587 0.413 1.88 ## stfdem 0.369 0.568 0.325 0.564 0.436 2.38 ## pltinvt 0.817 0.695 0.305 1.08 ## pltcare 0.811 0.695 0.305 1.11 ## trstplt 0.510 0.611 0.716 0.284 2.40 ## ## RC3 RC2 RC1 ## SS loadings 2.942 2.668 2.490 ## Proportion Var 0.226 0.205 0.192 ## Cumulative Var 0.226 0.432 0.623 ## Proportion Explained 0.363 0.329 0.307 ## Cumulative Proportion 0.363 0.693 1.000 ## ## Mean item complexity = 1.6 ## Test of the hypothesis that 3 components are sufficient. ## ## The root mean square of the residuals (RMSR) is 0.07 ## with the empirical chi square 15240.94 with prob &lt; 0 ## ## Fit based upon off diagonal values = 0.967 For attitude towards immigration, Kestilä extracted five components, with VARIMAX rotation: library(psych) pca_att &lt;- principal(df[, 20:44], nfactors = 5, rotate = &quot;varimax&quot;) print(pca_att, cut = .3) ## Principal Components Analysis ## Call: principal(r = df[, 20:44], nfactors = 5, rotate = &quot;varimax&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## RC2 RC1 RC5 RC3 RC4 h2 u2 com ## imsmetn 0.80 0.72 0.28 1.3 ## imdfetn 0.78 0.79 0.21 1.7 ## eimrcnt 0.83 0.71 0.29 1.1 ## eimpcnt 0.80 0.79 0.21 1.5 ## imrcntr 0.83 0.75 0.25 1.1 ## impcntr 0.78 0.78 0.22 1.6 ## qfimchr 0.82 0.70 0.30 1.1 ## qfimwht 0.76 0.65 0.35 1.3 ## imwgdwn 0.81 0.71 0.29 1.2 ## imhecop 0.75 0.67 0.33 1.4 ## imtcjob 0.57 0.34 0.48 0.52 2.0 ## imbleco 0.70 0.55 0.45 1.3 ## imbgeco 0.70 0.60 0.40 1.5 ## imueclt 0.57 -0.34 0.54 0.46 2.4 ## imwbcnt 0.67 0.63 0.37 1.9 ## imwbcrm 0.66 0.48 0.52 1.2 ## imrsprc 0.61 0.44 0.56 1.3 ## pplstrd 0.33 -0.54 0.46 0.54 2.2 ## vrtrlg -0.35 0.46 0.41 0.59 2.8 ## shrrfg 0.37 -0.35 0.42 0.58 4.1 ## rfgawrk 0.61 0.40 0.60 1.1 ## gvrfgap 0.69 0.56 0.44 1.3 ## rfgfrpc -0.39 0.33 0.67 3.3 ## rfggvfn 0.58 0.42 0.58 1.5 ## rfgbfml 0.60 0.46 0.54 1.6 ## ## RC2 RC1 RC5 RC3 RC4 ## SS loadings 4.38 3.40 2.78 2.19 1.72 ## Proportion Var 0.18 0.14 0.11 0.09 0.07 ## Cumulative Var 0.18 0.31 0.42 0.51 0.58 ## Proportion Explained 0.30 0.24 0.19 0.15 0.12 ## Cumulative Proportion 0.30 0.54 0.73 0.88 1.00 ## ## Mean item complexity = 1.7 ## Test of the hypothesis that 5 components are sufficient. ## ## The root mean square of the residuals (RMSR) is 0.05 ## with the empirical chi square 29520.06 with prob &lt; 0 ## ## Fit based upon off diagonal values = 0.98 5.0.8 Question 1.h Extract the PCA factor scores from the results objects, and add them to the data.frame. Give the PCA scores informative names, based on your interpretation of the factor loadings, so that you understand what they summarize. Hint: Use $; colnames(); cbind() Click for explanation Extracting factor scores The factor scores are INSIDE of the results objects. Use the $ operator to access them: head(pca_att$scores) ## RC2 RC1 RC5 RC3 RC4 ## 1 1.9920289 1.3140238 -0.8305392 -0.06329775 -0.08837693 ## 4 0.1708174 -1.2167781 -0.4974957 -0.23766146 0.67364069 ## 7 -0.3580985 0.3236336 -1.5094405 -0.53052720 -2.20637993 ## 14 NA NA NA NA NA ## 17 -0.1136716 -0.7869911 -1.4664715 -0.07112144 0.41078167 ## 20 -0.9188606 2.8264230 -0.3477484 -0.73788338 -1.32089442 We’re going to give these factor scores some informative names, and add them to our data.frame. You should give them different, informative names based on the meaning of the factors! # Print names colnames(pca_att$scores) ## [1] &quot;RC2&quot; &quot;RC1&quot; &quot;RC5&quot; &quot;RC3&quot; &quot;RC4&quot; # Change names colnames(pca_att$scores) &lt;- c(&quot;Att1&quot;, &quot;Att2&quot;, &quot;Att3&quot;, &quot;Att4&quot;, &quot;Att5&quot;) colnames(pca_trust$scores) &lt;- c(&quot;Trust1&quot;, &quot;Trust2&quot;, &quot;Trust3&quot;) # Add columns df &lt;- cbind(df, pca_trust$scores, pca_att$scores) 5.0.9 Question 1.i Are you able to replicate her results? Click for explanation No, probably not. 5.0.10 Question 1.j Save your syntax and bring your data and syntax to the practical on Thursday. "],
["week-3-home.html", "Chapter 6 Week 3 - Home", " Chapter 6 Week 3 - Home Last week, you have worked on the data used by Kestilä in a paper that discussed two possible reasons why there is no Radical Right party in Finland. You have attempted to 1) replicate her study by doing a Principal Component Analysis and 2) a factor analysis (exploratory) of the same data. This week you also learned that it is possible to do Confirmatory Factor Analysis within the structural equation modeling (SEM) framework. We use the R-package lavaan to fit these kinds of models. Before we will analyze the Kestilä data, you first need to learn some of the basic principles of doing analyses using lavaan. Using syntax, you need to tell lavaan exactly what kind of model you want it to estimate This opens up many more possibilities to do Theory Construction and then subsequently test your theory using Statistical Modeling. As a preparation for the next practical, work your way through this tutorial (part of which consists of the official lavaan tutorial). You will find that lavaan is a very user-friendly software package. 6.0.1 Get started with lavaan To get started with lavaan, read and run the following two chapters of the official lavaan tutorial: http://lavaan.ugent.be/tutorial/install.html http://lavaan.ugent.be/tutorial/syntax1.html (you just have to read this one) 6.0.2 Regression models in lavaan Download the data file Hamilton.xls from blackboard. The data are as follows: Hamilton (1990) provided several measurements on each of 21 states. Three of the measurements will be used in this tutorial: Average SAT score Per capita income expressed in $1,000 units Median education for residents 25 years of age or older Load the data from the xls file into R. Hint: Use XLConnect::readWorkSheetFromFile() Click for explanation library(XLConnect) df &lt;- readWorksheetFromFile(&quot;Hamilton.xls&quot;, 1) 6.0.3 Conceptual model The following path diagram shows a model for these data: This is a simple regression model where one observed variable, SAT, is predicted as a linear combination of the other two observed variables, Education and Income. As with nearly all empirical data, the prediction will not be perfect. The variable Other represents variables other than Education and Income that affect SAT. Each single-headed arrow represents a regression weight. The number 1 in the figure specifies that Other must have a weight of 1 in the prediction of SAT. This constraint is imposed by default in lavaan. 6.0.4 Lavaan syntax Based on the lavaan tutorial, write down (just as text) the model syntax that describes the model in the picture. How many regressions are there? How many covariances? Click for explanation The syntax for this model is: &quot;AT ~ Income + Education Income ~~ Education&quot; Or, equivalently: &quot;AT ~ Income AT ~ Education Income ~~ Education&quot; This syntax specifies two regression equations and one covariance. However, three more parameters are included by lavaan per default: The residual (unexplained) variance in SAT The variance of Income The variance of Education So, strictly speaking, if you don’t want to rely on the default settings, the syntax would be: &quot;SAT ~ Income + Education Income ~~ Education SAT~~SAT Income~~Income Education~~Education&quot; 6.0.5 Performing the analysis In lavaan, models are fit using the sem() function. Run the command ?sem to open the help file for this function. Try to figure out how to take the syntax you wrote for the previous question, and fit it to the Hamilton data. Click for explanation # Load the lavaan package library(lavaan) # Fit the model to df, and store the result in an object called &#39;fit&#39; fit &lt;- sem(model = &quot;SAT ~ Income + Education Income ~~ Education&quot;, data = df) 6.0.6 Viewing the output Most of the relevant output of a lavaan analysis can be extracted using the summary() function. Get a summary for the analysis now. Do either of the predictors have a significant effect on SA? By specifying the option rsquare = TRUE in the summary() function, you can additionally get squared multiple correlations for the dependent variables. Click for explanation summary(fit, rsquare = TRUE) ## lavaan 0.6-3 ended normally after 48 iterations ## ## Optimization method NLMINB ## Number of free parameters 6 ## ## Number of observations 21 ## ## Estimator ML ## Model Fit Test Statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Information Expected ## Information saturated (h1) model Structured ## Standard Errors Standard ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## SAT ~ ## Income 2.156 3.050 0.707 0.480 ## Education 136.022 29.819 4.562 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## Income ~~ ## Education 0.127 0.064 2.000 0.046 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .SAT 382.736 118.115 3.240 0.001 ## Income 2.562 0.791 3.240 0.001 ## Education 0.027 0.008 3.240 0.001 ## ## R-Square: ## Estimate ## SAT 0.603 6.0.7 Plotting the output The package semPlot can automatically plot simple SEM models, like path models and CFA models. To visualize this SEM model, install the semPlot package, and use the function semPaths: install.packages(&quot;semPlot&quot;) library(semPlot) semPaths(fit) The default plot can be improved upon, for example, by plotting the parameter estimates onto the paths, and rotating it to match our initial conceptual model at the start of this tutorial: semPaths(fit, whatLabels = &quot;est&quot;, rotation = 2) "]
]
