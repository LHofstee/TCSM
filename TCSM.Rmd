--- 
title: "Theory Construction and Statistical Modeling"
subtitle: "A guide to structural equation modeling in R"
author: "Caspar J. van Lissa¹"
date: "¹Utrecht University, Methodology & Statistics"
site: bookdown::bookdown_site
output: 
  bookdown::gitbook:
    config: 
      toc:
       collapse: section
      search: yes
      fontsettings:
        size: 2
    split_by: section
    df_print: paged
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
description: "This is a guide for the elective course TCSM."
---

# Course {-}

In this course you will learn how to translate a social scientific theory into a statistical model, how to analyze your data with these models, and how to interpret and report your results following APA standards.

The analyses will be executed using the statistical programming environment R, and in particular using the structural equation modeling package lavaan.

<!--chapter:end:index.Rmd-->

# Preparing for the course

![](chap2.jpg)

## Installing software 

Before we start the course, we have to install three things:

* **R**, a free program for statistical programming
* **RStudio**, a user interface which makes it easier to work with R; overlook our data, packages and output. 
* Several **packages**, which are 'add-ons' for R with functions to do specific analyses. They also include the documentation (help files) that describes how to use them, and sample data.

This Chapter shows how you can install RStudio on your computer. We'll also provide some general information on R, and how you can get help if you get error messages.

If you already have RStudio installed on your computer, and if you're an experienced R user already, all of this might be nothing new for you. You may **skip** this chapter then.

If you have **never used R before, this Chapter is essential**, as it gives you some input on how R works, and how we can use it for our data analyses.

<br><br>

---

### 1. Installing R

You will have to install the latest R Version, which is available [here](https://cran.r-project.org).

### 2. Installing RStudio

```{r, echo=FALSE, fig.width=3,fig.height=2}
library(png)
library(grid)
img <- readPNG("rstudiologo.PNG")
grid.raster(img)
```

Download RStudio on the **RStudio** Website ([Link](https://www.rstudio.com/products/rstudio/download/)). It's free!

### 3. Installing packages

As a prerequisite for this guide, you need to have a few essential **R packages** installed.

1. Open RStudio
2. Inside RStudio, find the window named **Console** on the bottom left corner of your screen (it might fill the entire left side of the screen).
3. We will now install a few packages using R Code. Here's an overview of the packages, and why we need them:

```{r,echo=FALSE}
library(kableExtra)
Package<-c("lavaan", "psych", "ggplot2")
Description<-c("A sophisticated and user-friendly package for structural equation modeling", "A package with convenience functions for screening data, computing scale scores, calculating reliability, etc", "A flexible and user-friendly package for making graphs")
m<-data.frame(Package,Description)
names<-c("Package", "Description")
colnames(m)<-names
kable(m)
```
  
<br><br>
4. To install these packages, we use the `install.packages()` function in R. One package after another, our code should look like this:

```{r, eval=FALSE}
install.packages("lavaan")
install.packages("psych")
install.packages("ggplot2")
```

```{block, type='rmdachtung'}
Don't forget to put the package names in `""`. Otherwise, you will get an error message.
```


<br><br>

### Get started

### Starting a new project in Rstudio
 
To keep all your work organized, you should use a **project**. In Rstudio, click *File > New Project > New directory > New project*. Type the desired directory name in the dialog (give it a meaningful name, e.g. "My Meta-Analysis"), and use 'Browse' if you need to change the directory where you store your projects. Now, in your project, click *File > New file > R script*. This script file works just like notepad, or the syntax editor in SPSS: You type plain text, but you can run it any time you want. Conduct all of the exercises in this script file.

### Code conventions

Throughout the guide, a consistent set of conventions is used to refer to code:

* Functions are in a code font and followed by parentheses, like
`sum()` or `mean()`.
* Other R objects (like data or function arguments) are in a code
font, without parentheses, like `seTE` or `method.tau`. 
* Sometimes, we’ll use the package name followed by two colons, like
`lavaan::sem()`. This is valid R code and will run. The `lavaan::` part indicates that the function `sem()` comes from the package `lavaan`.

### Getting Help

As you start to apply the techniques described in this guide to your data you will soon find questions that the guide does not answer. This section describes a few tips on how to get help.

1. Every function in R has documentation (a help file). To see it, select the name of the function and press F1, or run the command `?` followed by the name of the function, e.g.: `?aov`.
2. Andy Field, the book used for our undergraduate statistics courses [@field2012discovering], is also available for R. Many basic analyses are explained for R in this book.
3. If you get stuck, start with **Google**. Typically, adding “R” to a search is enough to restrict it to relevant results, e.g.: "exploratory factor analysis R". Google is particularly useful for error messages. If you get an error message and you have no idea what it means, try googling it. Chances are that someone else has been confused by it in the past, and there will be help somewhere on the web. (If the error message isn’t in English,
run `Sys.setenv(LANGUAGE = "en")` and re-run the code; you’re
more likely to find help for English error messages.)
4. If Google doesn’t help, try [stackoverflow](https://stackoverflow.com). Start by spending a little time searching for an existing answer; including [R] restricts your search to questions and answers that use R.
5. Lastly, if you stumble upon an error (or typos!) in this guide's text or R syntax, feel free to contact **Caspar van Lissa** at **c.j.vanlissa@uu.nl**.

<br><br>

---




<!--chapter:end:01-rstudio_and_basics.Rmd-->

# Getting your data into R


```{block,type='rmdinfo'}
This optional chapter will tell you about how you can **import** data in RStudio. We will also show you a few commands to **manipulate data** directly in R.

```

## Using R projects

One advantage of using an **R project** is that the project directory is automatically set as the working directory. Just copy your data file to the folder that contains the *".Rproj"* file, and you will be able to load files by name.

## Importing Excel Files

One way to get Excel files directly into R is by using the `XLConnect` package. Install the package, and try using the readWorksheetFromFile() function to load the data, and assign it to an object called `df`:

```{r, eval = FALSE}
# Run this only once, to download and install the package:
install.packages("XLConnect")
# Load the package:
library(XLConnect)
# Read an Excel file into 'df':
df <- readWorksheetFromFile("your_file.xlsx",
                            sheet = 1)
```

```{r, echo=FALSE, results = "hide", message=FALSE, eval = FALSE}
library(XLConnect)
df <- readWorksheetFromFile("happy.xlsx",
                            sheet = 1)
```

### Inspect the data

R does not work with a single spreadsheet (SPSS or Excel). Instead, it can keep many objects in memory. The object `df` is a `data.frame`; an object that behaves similar to a spreadsheet. To see a description of the object, look at the *Environment* tab in the top right of Rstudio, and click the arrow next to `df`.

```{r, echo=FALSE}
library(png)
library(grid)
img <- readPNG("environment.PNG")
grid.raster(img)
```

As you can see, the on the top-right pane **Environment**, your file is now listed as a data set in your RStudio environment.

You can make a quick copy of this data set by assigning the `df` object to a new object. This way, you can edit one, and leave the other unchanged. Assign the object `df` to a new object called `df_backup`:


```{r}
df_backup <- df
```

You can also have a look at the contents of `df` by **clicking** the object in the Environment panel, or running the command `head(df)`.

<br><br>

---


## Importing SPSS Files

SPSS files can be loaded using the `foreign` package. All SPSS files for this course are available on Blackboard.

<!--[here](https://github.com/cjvanlissa/Doing-Meta-Analysis-in-R/blob/master/Problem2.sav?raw=true).-->

```{r, eval = FALSE}
# Install the package, run this only once
install.packages("foreign")

# Load the `foreign` library
library(foreign)

# Read the SPSS data
df <- read.spss("sesam2.sav",
                     to.data.frame = TRUE)
```
```{r, echo = FALSE}
library(foreign)

# Read the SPSS data
df <- read.spss("sesam2.sav",
                     to.data.frame = TRUE)
```


## Data manipulation (optional)

Now that we have the Meta-Analysis data in RStudio, let's do a **few manipulations with the data**. These functions might come in handy when were conducting analyses later on.


Going back to the output of the `str()` function, we see that this also gives us details on the type of column data we have stored in our data. There a different abbreviations signifying different types of data.

```{r,echo=FALSE}
library(kableExtra)
Package<-c("num","chr","log","factor")
type<-c("Numerical","Character","Logical","Factor")
Description<-c("This is all data stored as numbers (e.g. 1.02)","This is all data stored as words","These are variables which are binary, meaning that they signify that a condition is either TRUE or FALSE","Factors are stored as numbers, with each number signifying a different level of a variable. A possible factor of a variable might be 1 = low, 2 = medium, 3 = high")
m<-data.frame(Package,type,Description)
names<-c("Abbreviation", "Type","Description")
colnames(m)<-names
kable(m)
```

### Converting to factors {#convertfactors}

Let's look at the variable `df$VIEWCAT`. This is a categorical variable, coded as a numerical one. We can have a look at this variable by typing the name of our dataset, then adding the selector `$` and then adding the variable we want to have a look at.
This variable is currently a numeric vector. We want it to be a factor: That's a categorical variable.

To convert this to a **factor** variable now, we use the `factor()` function.

```{r, results = "hide"}
df$VIEWCAT <- factor(df$VIEWCAT)
```
We now see that the variable has been **converted to a factor with the levels "1", "2", "3", and "4"**. We can assign different value labels as follows:

```{r, results = "hide"}
df$VIEWCAT <- factor(df$VIEWCAT, labels = c("Rarely", "Sometimes", "Regularly", "Often"))
```


### Selecting specific cases {#select}

It may often come in handy to **select certain cases for further analyses**, or to **exclude some studies in further analyses** (e.g., if they are outliers).

To do this, we can use the `[]` operator to index our data.

Let's say we want to get only the first 5 cases. We can select them like so:

```{r}
df[1:5, ]
```

Or let's say we only want the children younger than 36 months in the dataset. In this case, we can use **boolean indexing**: We create a TRUE / FALSE statement, and select the cases that are TRUE:

```{r}
df[df$AGE < 36, ]
```

Note that this approach can be used for any other type of data and variable. We can also use it to e.g., only select studies where VIEWCAT was equal to "Often" "typical":

```{r, eval = FALSE}
df[df$VIEWCAT == "Often", ]
```
```{r, echo = FALSE}
df[df$VIEWCAT == "Often", ][1:5, ]
```

### Changing cell values

Sometimes, even when preparing your data in EXCEL, you might want to **change values in RStudio once you have imported your data**. 

To do this, we have to select a cell in our data frame in RStudio. This can be done by adding `[x,y]` to our dataset name, where **x** signifies the number of the **row** we want to select, and **y** signifies the number of the **column**.

To see how this works, let's select a variable using this command first:

```{r}
df[8,1]
```

We now see the **8th study** in our dataframe, and the value of this study for **Column 1 (participant ID)** is displayed. Let's say we had a typo in this name and want to have it changed. In this case, we have to give this exact cell a new value.

```{r}
df[8,1] <- 1001
```

Let's check if the value has changed.

```{r}
df[8,1]
```

You can also use this function to change any other type of data, including numericals and logicals. Only for characters, you have to put the values you want to insert in `""`.

<br><br>

---


<!--chapter:end:02-getting_data_in_R.Rmd-->

# Week 1 - Home

Open the data file LifeSat.sav. 

```{r, message=FALSE}
library(foreign)
data <- read.spss("LifeSat.sav", to.data.frame = TRUE)
```

### Question 1.a

Make a descriptives table for the variables: LifSat, educ, ChildSup, SpouSup, and age. 

What is the average age in the sample? And the range (youngest and oldest child)? 

*Hint: Use* `library(psych); describe(); []`

<details>
  <summary>Click for explanation</summary>
The package `psych` contains many functions for exploring data. Install and load the package, then use the `describe()` function to describe the data:

```{r}
library(psych)
describe(data[, c("LifSat", "educ", "ChildSup", "SpouSup", "age")])
```
<\details>


### Question 1.b

Perform a simple regression with LifSat as the dependent variable and educ as the independent variable.

*Hint: The function* `lm()` *(short for linear model) conducts linear regression. The functions* `summary()` *provides relevant summary statistics for the model. It can be helpful to store the results of your analysis in an object, too.*

<details>
  <summary>Click for explanation</summary>

```{r}
results <- lm(LifSat ~ educ, data)
summary(results)
```
<\details>

### Question 1.c.

Do the same with age as the independent variable. 

<details>
  <summary>Click for explanation</summary>

```{r}
results <- lm(LifSat ~ age, data)
summary(results)
```
<\details>


### Question 1.d. 

Again with ChildSup as the independent variable. 

<details>
  <summary>Click for explanation</summary>

```{r}
results <- lm(LifSat ~ ChildSup, data)
summary(results)
```
<\details>


### Question 1.e.

Perform a multiple regression with LifSat as the dependent variable and educ, age and ChildSup as the independent variables. 

*Hint: You can use the + sign to add multiple variables to a model.*

<details>
  <summary>Click for explanation</summary>

```{r}
results <- lm(LifSat ~ educ + age + ChildSup, data)
summary(results)
```
<\details>


### Question 1.f. 

Compare the results under 1.e with those obtained under 1.b-1.d. What do you notice when you compare the regression parameter for each of the three predictors in the multiple regression with the corresponding regression parameters obtained in the simple regressions? 

<!--chapter:end:03-Week1_home.Rmd-->

# Week 1 - Class

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
show_answers <- TRUE
```

During the practical you will work on some exercises about ANOVA and ANCOVA using regression and path modeling. Note that ANOVA and ANCOVA are special cases of regression, as discussed during MTS3 or a similar course. How to perform an ANOVA/ANCOVA as a regression analysis is prerequisite knowledge. 

This practical we will work on these topics (ANOVA, ANCOVA, regression and how they are related). If you need to refresh your knowledge you could use the internet to find information or you could look it up in a book on statistics, for example @field2012discovering (*The chapters on ANOVA, Factorial ANOVA, and ANCOVA (11.6)*).

We start with two exercises in which you have to explore your data and perform a regression analysis, ANOVA and an ANCOVA. You will also practice with performing an ANCOVA as a regression analysis in exercise 3 today.


## Loading data

Open the file Sesam.sav:

```{r, message=FALSE}
# Library for reading SPSS files:
library(foreign)
# Load the data and put them in the object called "data"
data <- read.spss("sesam.sav", to.data.frame = TRUE, use.value.labels = FALSE)
```

This file is part of a larger dataset that evaluates the impact of the first year of the Sesame Street television series. Sesame Street is mainly concerned with teaching preschool related skills to children in the 3-5 year age range.

The following variables will be used in this exercise:

* **age** 	measured in months
* **prelet**	knowledge of letters before watching Sesame Street (range 0-58)
* **prenumb**	knowledge of numbers before watching Sesame Street (range 0-54)
* **prerelat**	knowledge of relations before watching Sesame Street (range 0-17)
* **peabody**	vocabulary maturity before watching Sesame Street (range 20-120)
* **postnumb** 	knowledge of numbers after a year of Sesame Street (range 0-54)

## Section 1

### Question 1.a

What is the level of measurement of each of the variables?

<details>
  <summary>Click for explanation</summary>
In the 'Environment' panel in the top right corner of the screen, click the arrow in the next to the object called 'data'. Alternatively, run the rode: `head(data)`.

```{r echo = FALSE}
knitr::include_graphics("measurement_level.png")#, out.width = "456px")
```
</details> 

### Question 1.b

What is the average age in the sample? And the range (youngest and oldest child)? 

*Hint: Use install.packages("psych"); library(psych); describe()*

<details>
  <summary>Click for explanation</summary>
The package `psych` contains many functions for exploring data. Install and load the package, then use the `describe()` function to describe the data:

```{r, eval = FALSE}
install.packages("psych")
library(psych)
describe(data)
```
```{r, echo=FALSE}
library(psych)
describe(data)
```
<\details>

### Question 1.c

What is the average gain in knowledge of numbers? Provide both the mean and the standard deviation. 

*Hint: Use the <- operator to assign to a new variable in data. Functions mean() and sd().*

<details>
  <summary>Click for explanation</summary>
Create a new variable that represents the difference between pre- and post-test scores:

```{r}
data$dif <- data$postnumb - data$prenumb
```

There are specialized functions to obtain the mean and sd:

```{r}
mean(data$dif)
sd(data$dif)
```
<\details>

### Question 1.d

Choose an appropriate graph to present the gain scores. What did you choose and why?

*Hint: Several useful plotting functions for univariate distributions are:* `hist(); plot(density()); boxplot()`

<details>
  <summary>Click for explanation</summary>
```{r include = show_answers}
plot(density(data$dif))
```
<\details>

### Question 1.e

Can you think of a graph based on two variables that is informative? What is it and how is it informative?

*Hint: A useful plotting function for a bivariate distribution is the scatterplot:* `plot(data$x, data$y)`

<details>
  <summary>Click for explanation</summary>
```{r include = show_answers}
#Possible variables would be the pre- and post measurement
plot(data$prenumb, data$postnumb)
```
<\details>


### Question 1.f

Which of the variables age, prelet, prenumb, prerelat and peabody are significantly related to postnumb? Use Pearson’s correlations (`cor()`). You don’t need to check assumptions.

*Hint: The function* `cor()` *provides Pearson's correlations. Select variables by name from a data.frame object (like* `data`*) using the following syntax:

<details>
  <summary>Click for explanation</summary>
```{r}
cor(data[, c("age", "prelet", "prenumb", "prerelat", "peabody", "postnumb")])
```

The use of `data[,]` follows the conventions of matrix indexation: You can select rows (the horizontal lines) like this, `data[i, ]`, and columns (the vertical lines) like this, `data[ ,j]`, where i are the rows and j are the columns you want to select. As you can see in the example, you can select multiple columns using c( ... , ... ).
<\details>

### Question 1.g

Can age and prenumb be used to predict postnumb? If so, discuss the substantial importance of the model and the significance and substantial importance of the separate predictors.

*Hint: The function* `lm()` *(short for linear model) conducts linear regression. The functions* `summary()` *provides relevant summary statistics for the model. It can be helpful to store the results of your analysis in an object, too.*

<details>
  <summary>Click for explanation</summary>
```{r}
results <- lm(formula = postnumb ~ age + prenumb, 
              data = data)
summary(results)
```
<\details>

### Question 1.h

Provide the null hypotheses and the alternative hypotheses of the model in 1.g.

<details>
  <summary>Click for explanation</summary>
The null-hypotheses of the **model** pertain to the variance explained: $\rho^2$ (that's Greek letter rho, for the population value of $\rho^2$).

$H_0: \rho^2 = 0$

$H_a: \rho^2 > 0$
<\details>

### Question 1.i

Consider the path model below. How many regression coefficients are estimated in this model? And how many variances? And how many covariances? How many degrees of freedom does this model have? ($df = N_{obs} – N_{par}$, see slides Lecture 1). 

```{r echo = FALSE}
knitr::include_graphics("1_path_model.png")#, out.width = "456px")
```

### Question 1.j

Consider a multiple regression analysis with three continuous independent variables, tests in language, history and logic, and one continuous dependent variable, a score on a math test. We want to know whether the various tests can predict the math score. Sketch a path model for this analysis (there are examples in the lecture slides of week 1). 

How many regression parameters are there? How many variances could you estimate? 
How many covariances could you estimate? How many degrees of freedom does this model have?

## Section 2

Open the file Drivers.sav.

### Research question 1 (ANOVA): Does talking on the phone interfere with people’s driving skills?

IV: condition

* hand-held phone
* hands-free phone 
* control

DV: reaction time in milliseconds in a driver simulation test.

```{r, message=FALSE}
# Load the data and put them in the object called "data"
data <- read.spss("Drivers.sav", to.data.frame = TRUE)
```

### Question 2.a

Perform the ANOVA. 

*Hint: The function* `aov()` *is an alternate interface to the linear model (lm), which reports results in line with the convention of ANOVA analyses.*

<details>
  <summary>Click for explanation</summary>
```{r, message=FALSE}
results <- aov(formula = RT ~ condition, data = data)
summary(results)
```
<\details>

### Question 2.b

What are the assumptions you need to check?

<details>
  <summary>Click for explanation</summary>
We can check several assumptions:

1. Presence of outliers
2. Normality of residuals
3. Homogeneity of residuals

Let's deal with them in order.

#### Presence of outliers:

**In Y-space**

We can check the range of the standardized (`scale()`) residuals for outliers in Y-space.  The residuals are **inside** of the results object, so we can just extract them, standardize them, and get the range:

```{r}
range(scale(results$residuals))
```

What is your conclusiong about the outliers?

#### Normality of residuals

We can check the normality of residuals using a QQplot.

```{r}
qqnorm(results$residuals)
qqline(results$residuals)
``` 

There appears to be some mild deviation from normality at the extremes.

You can also **test** for normality with the `shapiro.test(x)` function:

```{r}
shapiro.test(results$residuals)
``` 

#### Homogeneity of Variances

The `bartlett.test()` function provides a parametric K-sample test of the equality of variances. This test has the same hypotheses as the Levene's test.

```{r}
bartlett.test(formula = RT~condition, data = data)
```

It can also be nice to use a paneled boxplot to visualize the distributions. For this, we will use the package `ggplot2`:

```{r, eval = FALSE}
install.packages("ggplot2")
library(ggplot2)
ggplot(data, aes(y = RT, group = condition)) +
  geom_boxplot() +
  theme_bw()
```


```{r, echo = FALSE}
library(ggplot2)
ggplot(data, aes(y = RT, group = condition)) +
  geom_boxplot() +
  theme_bw()
```
<\details>

### Question 2.c

Explain for each of the assumptions why they are important to check.

### Question 2.d

What are your conclusions regarding the assumption checks?

<details>
  <summary>Klik voor meer uitleg</summary>
    There are no outliers in X-space, no evidence for (severe) deviations from normality of residuals, and no evidence for (severe) heteroscedasticity.
</details> 

### Question 2.e

Answer the research question. 

*Hint: Use* `summary()` *and* `TukeyHSD()`*.*

<details>
  <summary>Click for explanation</summary>
  We can examine the overall F-test, which is significant:
  
```{r, message=FALSE}
summary(results)
```

Post-hoc tests with Bonferroni correction can be obtained using `TukeyHSD(results)`. We notice that none of these comparisons are significant. However, the research question was *Does talking on the phone interfere with peoples driving skills?* There are two conditions for talking on the phone. We could thus test a planned contrast of these two conditions against the control condition, instead of all possible post-hoc tests:

The standard contrasts are dummy coded:

```{r, message=FALSE}
contrasts(data$condition)
```

We can replace these with planned contrasts for "phone" vs control, and hand-held vs hands-free:

```{r, message=FALSE}
contrasts(data$condition) <- cbind(phoneVcontrol = c(-1, -1, 2), handVfree = c(-1, 1, 0))
results <- aov(RT ~ condition, data)
# Ask for the lm summary, which gives you t-tests for the planned contrasts:
summary.lm(results)
```
<\details>

### Research question 2 (ANCOVA): Are there differences in reaction time between the conditions when controlling for age? 


### Question 2.f

What are the assumptions you need to check?

<details>
  <summary>Click for explanation</summary>
  Assumptions for ANCOVA are the same as for ANOVA (no outliers, normality of residuals, homoscedasticity). ANCOVA has the following additional assumptions:

* Homogeneity of regression slopes for the covariate (no interaction between factor variable and covariate)
* The covariate is independent of the treatment effects. I.e. there is no difference in the covariate between the groups of the independent variable.

<\details>


### Question 2.g

Explain for each of the assumptions why they are important to check.

### Question 2.h

Check the assumptions of ANCOVA.

*Hint: Within formulas, you can use* `*` *instead of* `+` *to include interaction effects.*


<details>
  <summary>Click for explanation</summary>
  
#### Homogeneity of regression slopes

Add the interaction to the model and test whether the interaction is significant:

```{r}
results_age <- aov(RT ~ condition + age, data)
results_age_int <- aov(RT ~ condition * age, data)
summary(results_age_int)

#Or you could use `anova()` to compare two different models
anova(results_age, results_age_int)
```

What would your conclusion be about this assumption?

<details>
  <summary>Click for explanation</summary>
The interaction is NOT significant; no evidence for violation of the assumption.
<\details>

#### The covariate is independent of the treatment effects

```{r}
results_indep <- aov(age ~ condition, data)
summary(results_indep)
```
What would your conclusion be about this assumption?

<details>
  <summary>Click for explanation</summary>
The covariate is not significantly related to treatment effect. The assumption is met.
<\details>

<!--#### Outliers in X-space

In addition to the aforementioned outliers in Y-space, we can now test for (multivariate) outliers in X-space using Mahalanobis' distance. In this case, we only have one continuous covariate. The function requires a **matrix** of data, a vector of means for centering the data, and a covariance matrix. In the syntax below, we use `drop = FALSE` when extracting a single column from the data, in order to make sure that the data will still be in matrix format when we extract only one column. This is necessary for the underlying matrix algebra. We have to take the `sqrt()` because the function `mahalanobis()` returns the **squared** Mahalanobis' distances.

```{r}
mahal <- sqrt(mahalanobis(data[ , "age", drop = FALSE], 
                          center = mean(data$age),
                          cov = cov(data[, "age", drop = FALSE])))
range(mahal)
```
<\details>
-->

### Question 2.i

Answer the research question. (Do you have to include the interaction or not?)


<details>
  <summary>Click for explanation</summary>
```{r, message=FALSE}
results <- aov(formula = RT ~ condition + age, data = data)
TukeyHSD(results)
```

The handheld-condition has a significant **higher** reaction time than the control condition

<\details>


## Section 3

Open the file Sesam2.sav. 


```{r, message=FALSE}
# Load the data and put them in the object called "data"
data <- read.spss("Sesam2.sav", to.data.frame = TRUE)
```

Use postnumb as the dependent variable in all the following analyses.

### Question 3.a

Viewcat is a factor variable, but is not coded as such in the data. Turn it into a factor. Afterwards, make sure that viewcat=1 is the reference group in the contrasts, i.e., the group that is identified by zero scores on all the associated dummy variables.

*Hint: Use* `<- factor()` *and* `contrasts()`.

<details>
  <summary>Click for explanation</summary>
  
```{r}
data$VIEWCAT <- factor(data$VIEWCAT)
contrasts(data$VIEWCAT)
```

<\details>

### Question 3.b

Perform a multiple regression analysis with just the viewcat dummies as predictors.

<details>
  <summary>Click for explanation</summary>
  
```{r}
results <- lm(POSTNUMB ~ VIEWCAT, data)
summary(results)
```
<\details>

### Question 3.c

What do the regression coefficients represent? How can you determine the average postnumb score for each of the viewcat categories, based on the regression parameters?

### Question 3.d

Make a coloured scatter plot with age on the x-axis and postnumb on the y-axis. Colour the dots according to the their `viewcat` category. How do you interpret the differences in slopes of these four fit lines?

*Hint: Use* `ggplot(); geom_point(); geom_smooth()`. *Within ggplot, use the aes(colour = '...') to colour according to a certain variable.*

<details>
  <summary>Click for explanation</summary>
  We will use ggplot again:
  
```{r}
ggplot(data, aes(x = AGE, y = POSTNUMB, colour = VIEWCAT)) + 
  geom_point() + # For scatterplot
  geom_smooth(method = "lm", se = FALSE) + # For regression lines
  theme_bw() # For a pretty theme
```
<\details>

### Question 3.e

Add an interaction between age and viewcat to the regression analysis. 

*Hint: An interaction is created by multiplying two variables. You can multiply with \* in the formula of* `lm()`.

<details>
  <summary>Click for explanation</summary>
```{r}
results_interaction <- lm(POSTNUMB ~ VIEWCAT*AGE, data)
summary(results_interaction)
```
<\details>

### Question 3.f

Perform a sequential multiple regression. Include age and viewcat as the predictors in the first analysis. Add the interaction term in the second analysis. Make sure to obtain information about the change in R-square!

*Hint: Use* `anova()` *to compare two regression models.*

<details>
  <summary>Click for explanation</summary>
```{r}
results_main <- lm(POSTNUMB ~ VIEWCAT + AGE, data)
anova(results_main, results_interaction)
```
<\details>

### Question 3.g

Sketch path models of both steps of the regression analysis (on paper). 

### Question 3.h

Write down the regression equations of both steps of the sequential analysis. 


<details>
  <summary>Click for explanation</summary>
$Postnumb_i = b_0 + b_1D_{view2i} + b_2D_{view3i} + b_3D_{view4i} + b_4Age_i + \epsilon_i$

$Postnumb_i = b_0 + b_1D_{view2i} + b_2D_{view3i} + b_3D_{view4i} + b_4Age_i + b_5D_{view2i}Age_i + b_6D_{view3i}Age_i + b_7D_{view4i}Age_i + \epsilon_i$

<\details>

### Question 3.i

Write down the null hypothesis that is tested to determine whether there is an interaction between age and viewcat.

<details>
  <summary>Click for explanation</summary>
$H_0: \Delta\rho^2 = 0$
<\details>

### Question 3.j

Indicate for each parameter in the second regression model what it means. Also write down the regression equation for each of the four categories of viewcat separately.

### Question 3.k

What do you conclude about the interaction between age and viewcat?

### Question 3.l

Note that you can also look at this problem as an ANCOVA. What are the research question and null hypothesis in this case?

<details>
  <summary>Click for explanation</summary>
RQ: Is there a significant difference between the marginal means of postnumb by viewcat, after controlling for age?

$H_0:$ After controling for age, the mans of postnumb are equal in all groups.

<\details>

### Question 3.m

Perform this analysis as an ANCOVA.

*Hint: Add* `-1` *to a formula to drop the intercept.*
<details>
  <summary>Click for explanation</summary>
  To drop the intercept from the analysis, and estimate the marginal means for all viewcat categories, we can add `-1` (minus the intercept) to the formula:
  
```{r}
results_ancov <- aov(POSTNUMB~AGE+VIEWCAT-1, data)
```
<\details>

Examine the parameter estimates of the ANCOVA. What do the parameter estimates represent?

<details>
  <summary>Click for explanation</summary>
We use summary.lm() again to obtain the parameter estimates:

```{r}
summary.lm(results_ancov)
```
The parameter estimates are the means of each VIEWCAT category when age = 0.

<\details>

<!--chapter:end:04-Week1_class.Rmd-->

# Week 2 - Home

This exercise is based on Kestilä, Elina (2006) Is There Demand for Radical Right 
Populism in the Finnish Electorate? Scandinavian Political Studies 29(3),169-191 
 
You have read and answered questions about the article in the reading questions. In this 
exercise, as well as in the second class practical, we will analyze these data ourselves. 

The data for this practical stem from the first round of the European Social Survey (ESS). 
This is a repeated cross-sectional survey across 32 European countries. The first round 
was held in 2002, and since then, subsequent rounds of data-collection are held bi-
anually. More info, as well as access to all data -> www.europeansocialsurvey.org. 

The raw, first round data can also be found on blackboard. The file is called ESSround1-
a.sav. This file contains data for all respondents, but only those variables are included 
that you will need in this exercise. 

### Question 1.a

Download the file, and import it in R. Inspect the file (no. of cases and no. of variables) to see if the file opened well. 

```{r, message=FALSE, warning=FALSE}
library(foreign)
data <- read.spss("ESSround1-a.sav", to.data.frame = TRUE)
```

<details>
  <summary><b>For a description of all variables in the dataset, click here!</b></summary>
```{r, results = "asis", echo = FALSE}
desc <- c("Title of dataset",
"ESS round",
"Edition",
"Production date",
"Country",
"Respondent's identification number",
"Trust in the legal system",
"Trust in the police",
"Trust in the United Nations",
"Trust in the European Parliament",
"Trust in country's parliament",
"State of health services in country nowadays",
"State of education in country nowadays",
"How satisfied with present state of economy in country",
"How satisfied with the national government",
"How satisfied with the way democracy works in country",
"Politicians interested in votes rather than peoples opinions",
"Politicians in general care what people like respondent think",
"Trust in politicians",
"Allow many/few immigrants of same race/ethnic group as majority",
"Allow many/few immigrants of different race/ethnic group from majority",
"Allow many/few immigrants from richer countries in Europe",
"Allow many/few immigrants from poorer countries in Europe",
"Allow many/few immigrants from richer countries outside Europe",
"Allow many/few immigrants from poorer countries outside Europe",
"Qualification for immigration: christian background",
"Qualification for immigration: be white",
"Average wages/salaries generally brought down by immigrants",
"Immigrants harm economic prospects of the poor more than the rich",
"Immigrants take jobs away in country or create new jobs",
"Taxes and services: immigrants take out more than they put in or less",
"Immigration bad or good for country's economy",
"Country's cultural life undermined or enriched by immigrants",
"Immigrants make country worse or better place to live",
"Immigrants make country's crime problems worse or better",
"Richer countries should be responsible for accepting people from poorer countries",
"Better for a country if almost everyone share customs and traditions",
"Better for a country if a variety of different religions",
"Country has more than its fair share of people applying refugee status",
"People applying refugee status allowed to work while cases considered",
"Government should be generous judging applications for refugee status",
"Most refugee applicants not in real fear of persecution own countries",
"Financial support to refugee applicants while cases considered",
"Granted refugees should be entitled to bring close family members",
"Gender",
"Year of birth",
"Highest level of education",
"Years of full-time education completed",
"How interested in politics",
"Placement on left right scale")

library(kableExtra)

kable(data.frame(Variable = names(data), Description = desc)) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))

```

### Question 1.b

The ESS-file contains much more information than we need to re-analyze the paper by 
Kestilä. We need to reduce the number of cases, in order to make the file more 
manageable, and make sure our results pertain to our target population.  

Kestilä only uses data from ten countries: `c("Austria", "Belgium", "Denmark", "Finland", "France", "Germany", "Italy", "Netherlands", "Norway", "Sweden")`.

Select data from these countries by means of boolean indexing, using the `%in%` function.

*Hint: Use* `[]; %in%`

<details>
  <summary>Click for explanation</summary>
```{r}
df <- data[data$cntry %in% c("Austria", "Belgium", "Denmark", "Finland", "France", "Germany", "Italy", "Netherlands", "Norway", "Sweden"), ]
```
\details


### Question 1.c

Inspect the data file again to see whether step 1b went ok.

### Question 1.d 

Before we can start the analyses, we first need to screen the 
data. What are the things we need to watch for? (think about your earlier 
statistics-courses)?

<details>
  <summary>Click for explanation</summary>
  This question is open to interpretation. One thing you might notice is that all variables the authors used are currently coded as factor variables (e.g., "Factor w/ 11 levels"):

```{r}
levels(df$trstlgl)
```

In keeping with conventions, we could treat ordinal Likert scales with >5 levels as continuous. We can either re-code the data, or prevent `read.spss()` from coding these variables as factors when it reads the data. Here is code for both approaches.

#### Re-coding factors to numeric

Data.frames like `df` are (secretly) lists, where each column is an element of the list. That means we can index a data.frame as a list, not just as a matrix. We can also use the incredibly powerful `lapply()` function, short for list apply, which takes each list element (column), and applies a function to it. In this case, that function is `as.numeric()`, which turns factors into numbers: 

```{r}
df[7:44] <- lapply(df[7:44], as.numeric)
table(df[,7])
```

#### Reading data without coding factors

An alternative solution is to stop the function `read.spss()` from using value labels to code variables as factors. However, we're not going to use this right now, so the information below is merely illustrative:

```{r, eval = FALSE}
# The option use.value.labels = FALSE stops the function from coding factors:
data <- read.spss("ESSround1-a.sav", to.data.frame = TRUE, use.value.labels = FALSE)
# Then, re-select the subset of data. The countries are now also unlabeled, so
# we select them by number:
df <- data[data$cntry %in% c(21,18,17,15,9,8,6,5,2,1), ]
```

\details

### Question 1.e

Screen all your variables univariately (one by one) by making frequency tables for 
every variable. Again, unsure how to do this? Review Field. Pay particular 
attention to the measurement scales of each variable. 

*Hint: Use* `table(); lapply(); psych::describe()`

<details>
  <summary>Click for explanation</summary>
  We can use the `lapply()` function from the previous question to get frequency tables for all relevant variables:

```{r, eval = FALSE}
lapply(df[7:44], table)
```

```{r, echo = FALSE}
lapply(df[7:8], table)
```

This is probably a bit too much detail, so we can just use the `psych::describe()` fuction again:

```{r, eval = FALSE}
psych::describe(df[7:44])
```

```{r, echo = FALSE}
psych::describe(df[7:16])
```
\details

### Question 1.f

Are there any incorrectly coded missing value labels, or other inexplicable values?

### Question 1.g

The first step in re-analyzing data is replicating the results from the paper by 
Kestilä. Run a Principal Component Analysis using `psych::principal()`, and 
choose the exact same specification as Kestilä concerning estimation method, 
rotation etc. Do two analyses: one for trust in politics, and one for attitudes towards immigration. 

*Hint: Use* `psych::principal()`

<details>
  <summary>Click for explanation</summary>
  There's a convenient function for PCA in the `psych` package, although base R also has the function `princomp()`.


#### Trust in politics

Kestilä extracted three components, with VARIMAX rotation. When we print the results, we can hide all factor loadings smaller than the smallest one in their table, to make it easier to read:
  
```{r}
library(psych)
pca_trust <- principal(df[, 7:19], nfactors = 3, rotate = "varimax")
print(pca_trust, cut = .3, digits =3)
```

For attitude towards immigration, Kestilä extracted five components, with VARIMAX rotation:
  
```{r}
library(psych)
pca_att <- principal(df[, 20:44], nfactors = 5, rotate = "varimax")
print(pca_att, cut = .3)
```
</details>

### Question 1.h

Extract the PCA factor scores from the results objects, and add them to the data.frame. Give the PCA scores informative names, based on your interpretation of the factor loadings, so that you understand what they summarize. 

*Hint: Use* `$; colnames(); cbind()`

<details>
  <summary>Click for explanation</summary>
Extracting factor scores

The factor scores are INSIDE of the results objects. Use the `$` operator to access them:

```{r}
head(pca_att$scores)
```

We're going to give these factor scores some informative names, and add them to our data.frame. **You should give them different, informative names based on the meaning of the factors!**

```{r}
# Print names
colnames(pca_att$scores)
# Change names
colnames(pca_att$scores) <- c("Att1", "Att2", "Att3", "Att4", "Att5")
colnames(pca_trust$scores) <- c("Trust1", "Trust2", "Trust3")
# Add columns
df <- cbind(df, pca_trust$scores, pca_att$scores)
```

\details


### Question 1.i

Are you able to replicate her results? 

<details>
  <summary>Click for explanation</summary>
  No, probably not. 

\details


### Question 1.j

Save your syntax and bring your data and syntax to the practical on Thursday.


<!--chapter:end:05-Week2_home.Rmd-->

# Week 2 - Class
During this exercise, you will perform multiple factor analyses. Before you will be able to execute these, you will have to install the R-package `GPArotation`.

``` {r, eval = FALSE}
install.packages("GPArotation")
```

We might add some kind of introduction to the class exercises of week 2.


## Loading data

In the unlikely event that you were able to replicate the results of Kestilä, load the environment of the take-home exercise of week 2. In case you have not saved the environment, you can just load and re-run your script of the take-home exercise.

If you did not manage to replicate the results, go to blackboard and download ESSround1-b.sav and load this dataset into RStudio. 

<details>
  <summary>Click for explanation</summary>

``` {r, message = FALSE, warning = FALSE}
library(foreign)
data <- read.spss("ESSround1-b.sav", to.data.frame = TRUE)
```

\details

### Question 1

Kestilä states that running a Principal Components Analysis is a good way to test whether the survey questions in the ESS measure attitudes towards immigration and trust in politics.
What would Preacher and MacCallum (2003) say about this?

<details>
  <summary>Click for information</summary>
  
*Hint: If you are working in R studio, you can simply add an `#` before each line of your answer, so that you will have all information inside the same document, without R thinking that it will have to run the code.

```{r}
#Simply like
#this
```

\details


### Question 2

If you would have to choose a method for constructing the 'trust in politics' and 'attitude towards immigration' scales based on the theory and background information in the Kestilä article, what type of factor analysis would you choose?
Think about:

* Estimation method
* Rotation method
* Method to establish how many factors are needed

### Question 3

Run two factor analyses, one for each PCA of the original article. Inspect the number of factors necessary, evaluate the rotation method, and if necessary, run the factor analysis again with adapted settings (rotation method and/or different number of factors). How many factors are there?

<details>
  <summary>Click for explanation</summary>

First, you will have to create a new data object containing only the countries of interest in which all variables are numeric.

```{r}
df <- data[data$cntry %in% c("Austria", "Belgium", "Denmark", "Finland", "Germany", "Italy", "Netherlands", "Norway", "Sweden"), ]
```

However, using this command, R will still see the excluded countries as factor levels, which will result into problems when you are going to replicate the study by Kestilä. To drop these factor levels completely, use the following command.

```{r}
df$cntry <- factor(df$cntry)
```

You also want to adjust the variables of interest to numeric once again.

```{r}
df[7:44] <- lapply(df[7:44], as.numeric)
```

To perform an exploratory factor analysis, you can use the function `fa` of the package `psych`. You will have to specify the data, and the variables that you want to include in the factor analyses. Furthermore, you will have to specify the number of factors that you want to extract, the rotation method and the estimation method.

```{r}
library(psych)
library(GPArotation)

efa_trust <- fa(df[, 7:19], nfactors = 3, rotate = "promax", fm = "ml", scores = "Bartlett")
```

In order to determine the number of factors to extract, you might want to look at the eigenvalues of the factors or the scree plot. These information can be accessed by using the following code:

```{r}
round(efa_trust$values, digits = 3)
plot(efa_trust$values, type = "b")
```

You will have to do the same for the attitude variables. 

```{r, warning = FALSE}
library(psych)
library(GPArotation)

efa_att <- fa(df[, 20:44], nfactors = 5, rotate = "promax", fm = "ml", scores = "Bartlett")
```

And for information about the number of factors, you can use the following commands.

```{r}
round(efa_att$values, digits = 3)
plot(efa_att$values, type = "b")
```

\details



### Question 4

Apart from the number of factors, you also want to look at the factor loadings. They can be found in the "pattern matrix". The higher the factor loadings are, the more indicative an item is for the latent factor. If you find some items to have only very low loadings (indicating that the items do not provide much information about the factor), you may choose not to include them in your analysis. This means you have to rerun the analysis under question 3. 

<details>
  <summary>Click for explanation</summary>
  
  You can find the factor loadings by means of the 'print'-function used in the take-home exercise, or you can search for the variable 'loadings', which is inside the results object, to end up with just the information you are searching for.  
  
  
```{r}
efa_trust$loadings
efa_att$loadings
```

The output above might appear slightly confusing, due to the jumble of factor loadings. To create more clarity, it is convenient to suppress the factor loadings that are lower than .30.

```{r}
print(efa_trust$loadings, cut = .30, digits = 2)
```

Furthermore, if you want to perform a factor analysis without, say, `trstep`, while you want all other variables included in your factor analysis, you can simply leave the column number of `trstep`, which is 13, out of the command:

```{r, eval = FALSE}
library(psych)
library(GPArotation)
efa_trust_without_stfedu <- fa(df[, c(7:12, 14:19)], nfactors = 3, rotate = "promax", fm = "ml")
```
\details


### Question 5

Give the factor scores an appropriate name. You can do this by inspecting the items that load on one factor. What do these items have in common substantively? The goal of a factor analysis usually is to create interpretable factors. If you have trouble interpreting the factors, you can choose to tweak the analysis by changing the options, or including/excluding more items.

Furthermore, after you named the factor scores accordingly, extract them from the results object and add them to the data.frame.

*Hint: If you do not know how to do this, have a look at question 1.h from the take-home exercise of week 2.*

**Please note that the `colnames` will be specified from left to right, and not, for example, from ML1 to ML5.**

```{r, echo = FALSE}
# Change names
colnames(efa_trust$scores) <- c("trustpolEFA", "satcntryEFA", "trustinstEFA")
colnames(efa_att$scores) <- c("effectimmiEFA", "allowimmiEFA", "allowrefEFA", "ethnicEFA", "immieurEFA")
# Add columns
df <- cbind(df, efa_trust$scores, efa_att$scores)
```

### Question 6

The next step is to assert whether the items that together form one factor, also form a reliable scale. Run separate reliability analyses by means of the function `alpha` for the items that together form one factor, and evaluate Cronbach's alpha to see whether the scales are internally consistent. The "Reliability if an item is dropped (alpha.drop)" information may be handy to inspect what would happen if you would delete one item; you can find it inside the reliability analysis object. If Cronbach's alpha is not ok, deselect one survey item and run the analyses under question 4 and question 5 again.

*Hint: Cronbach's alpha > .7 are deemed to be ok, > .8 is good.* 

If Cronbach's alpha is not ok, deselect one survey item and run the analyses under question 4 and question 5 again.

<details>
  <summary>Click for explanation</summary>

If you want to assess the reliability of the variables `pltcare`, `pltinvt`, `trstprl`, `trstplt`, and `trstep` you can run a reliability analysis as follows. 

*Hint: name the new objects substantively, instead of numbering them.*

```{r}
library(psych)
reli_1 <- psych::alpha(df[, c(7, 8, 9, 12, 13)])
reli_1
```

In the hypothetical scenario that Cronbach's alpha increases when you drop a variable out of the analysis, you can rerun your analysis without that specific variable. So for a version without variable `trstprl`, see the example below.

```{r}
library(psych)
reli_1b <- psych::alpha(df[, c(7, 8, 12, 13)])
```


\details

### Question 7

Now you can analyze the differences between the factor scores for the PCA analysis (take-home exercise 2) and the EFA by plotting them in a series of scatterplots (bivariate). This can be done by means of the package `ggplot2`. The PCA factor scores are already stored in the dataset `ESSround1-b.sav`.

<details>
  <summary>Click for explanation</summary>

*Make sure to adjust the variable names to the variables names of your own.*

```{r, eval = FALSE}
library(ggplot2)
ggplot(data = df) +
  geom_point(mapping = aes(x = EFA_trust1, y = PCA_Trust1))
```

```{r, echo = FALSE, warning = FALSE, message = FALSE}
library(ggplot2)
ggplot(data = df) +
  geom_point(mapping = aes(x = trustinstEFA, y = trustinst)) +
  labs(x = "EFA_Trust1",
       y = "PCA_Trust1")
```

\details

### Question 8

Build correlation matrices (PCA and EFA versions) of first the 'trust in politics' scores, and then the 'immigration' factor scores. What is your conclusion: is there a difference between them?

<details>
  <summary>Click for explanation</summary>
  
*Hint: Name the new objects substantively once again.*

```{r, eval = FALSE}
cor_trust_factor1 <- cor(df$EFA_Trust1, df$PCA_Trust1, use = "complete.obs")
cor_trust_factor1
```

```{r, echo = FALSE, warning = FALSE}
cor_trust_factor1b <- cor(df$trustinstEFA, df$trustinst, use = "complete.obs") 
cor_trust_factor1b
```

\details


### Question 9

Kestila uses the PCA factor scores to evaluate country level differences in 1. Attitudes towards immigration and 2. Political trust. Repeat her analyses using the factor scores you saved in step 5. Think about the statistical test you would like to use. Do you draw similar or different conclusions?

<details>
  <summary>Click for explanation</summary>

We can use an ANOVA to test whether the countries differ in the amount of political trust the participants have.
```{r}
fit_ANOVA <- aov(trustinstEFA ~ cntry, data = df)
summary(fit_ANOVA)
```

In which `trustinstEFA` is the dependent variable, `cntry` is the grouping variable, and `df` is the name of the dataset; `summary` will provide the results. However, it will only indicate whether the variable `cntry` is significant or not, so you will be unable to tell which countries differ in terms of their political trust. To tell which countries differ, you can do a pairwise comparison test, with a Bonferroni adjustment for multiple testing.


```{r}
pair_comparison <- pairwise.t.test(df$trustinstEFA, df$cntry, p.adjust.method = "bonf", na.rm = TRUE)
round(pair_comparison$p.value, digits = 3)
```


\details

### Question 10

The second goal of Kestilä is to show how socio-demographic characteristics affect attitudes towards immigrants and trust in politics in Finland. Select only the Finnish cases using the variable `cntry`. Next, run a number of multiple linear regression analyses with the (sub-)scales of attitudes towards immigrants and political trust as subsequent dependent variables, and the same predictors as Kestilä. Inspect your output.

Compare your results with the results from Kestilä. How do your results differ or agree with the results by Kestilä?

<details>
  <summary>Click for explanation</summary>
  
To select the Finnish cases only:
```{r}
df_finland <- df[df$cntry %in% "Finland", ]
df_finland$cntry <- factor(df_finland$cntry)
```

Although we have selected the Finnish cases, we still have to do some recoding. Since not the age of the participants is listed, but the year of birth, we have to recode this variable. Due to the fact that yrbrn was originally coded as a factor, we first have to create a numerical object based on the factor labels. Since the data originated from 2002, we can simply extract the year of birth of every participant from the year 2002.

The same holds for the variables `eduyrs` and `lrscale`, to prevent R from making dummies for all distinct values.

```{r}
df_finland$age <- as.numeric(as.character(df_finland$yrbrn))
df_finland$age <- (2002 - df_finland$age)

df_finland$eduyrs <- as.numeric(df_finland$eduyrs)
df_finland$lrscale <- as.numeric(df_finland$lrscale)
```

Fortunately, R creates a dummy for the variables `gndr` and `polintr` automatically. However, if you want to run the same analysis as Kestilä, you will have to recode `polintr` in such a way that there are only two categories, since R will go for a solution with one dummy per category by default. After you have done the recoding, you can replicate Kestiläs analysis.

```{r}
df_finland$polintr_dummy <- as.numeric(df_finland$polintr)
df_finland$polintr_dummy <- as.numeric(df_finland$polintr_dummy <= 2)
df_finland$polintr_dummy <- factor(df_finland$polintr_dummy,
                                   levels = c(0,1),
                                   labels = c("Not or hardly interested",
                                              "Quite or very interested"))
```

```{r}
fit_trust1 <- lm(trustinstEFA ~ gndr + age + eduyrs + polintr_dummy + lrscale, data = df_finland, na.action = na.omit)
summary(fit_trust1)
```
\details


### Question 11

Save your syntax and your data, you will need it next week.

<!--chapter:end:06-Week2_class.Rmd-->

# Week 3 - Home

Last week, you have worked on the data used by Kestilä in a paper that discussed two possible 
reasons why there is no Radical Right party in Finland. You have attempted to 1) replicate her 
study by doing a Principal Component Analysis and 2) a factor analysis (exploratory) of the same 
data. 

This week you also learned that it is possible to do Confirmatory Factor Analysis within the structural 
equation modeling (SEM) framework.  We use the R-package `lavaan` to fit these kinds of models. 
Before we will analyze the Kestilä data, you first need to learn some of the basic principles of doing 
analyses using `lavaan`. Using syntax, you need to tell `lavaan` exactly 
what kind of model you want it to estimate This opens up many more possibilities to do Theory Construction and then 
subsequently test your theory using Statistical Modeling. 

As a preparation for the next practical, work your way through this tutorial (part of which consists of the official `lavaan` tutorial). You will find that `lavaan` is a very user-friendly software package.

### Get started with lavaan

To get started with lavaan, read and run the following two chapters of the official `lavaan` tutorial:

* [Installing lavaan](http://lavaan.ugent.be/tutorial/install.html)
* [Lavaan syntax](http://lavaan.ugent.be/tutorial/syntax1.html) (you just have to read this one)

### Regression models in lavaan

Download the data file *Hamilton.csv*, or *Hamilton.xls* from blackboard. The data are as follows:

Hamilton (1990) provided several measurements on each of 21 states. Three of the
measurements will be used in this tutorial:

1. Average SAT score
2. Per capita income expressed in $1,000 units
3. Median education for residents 25 years of age or older

Load the data from the .csv or .xls file into R.

*Hint: Use* `read.csv()` *or* `XLConnect::readWorkSheetFromFile()`

<details>
  <summary>Click for explanation</summary>
```{r, message=FALSE}
library(XLConnect)
df <- readWorksheetFromFile("Hamilton.xls", 1)
```

Or 

```{r, eval=FALSE}
df <- read.csv("Hamilton.csv")
```
\details

### Conceptual model

The following path diagram shows a model for these data:

![](AMOS_path.png)

This is a simple regression model where one observed variable, SAT, is predicted as a
linear combination of the other two observed variables, Education and Income. As with
nearly all empirical data, the prediction will not be perfect. The variable Other
represents variables other than Education and Income that affect SAT.
Each single-headed arrow represents a regression weight. The number 1 in the
figure specifies that Other must have a weight of 1 in the prediction of SAT. This constraint is imposed by default in `lavaan`.

### Lavaan syntax

Based on the `lavaan` tutorial, write down (just as text) the model syntax that describes the model in the picture. How many regressions are there? How many covariances?

<details>
  <summary>Click for explanation</summary>
The syntax for this model is:

```{r, eval = FALSE}
"SAT ~ Income + Education
Income ~~ Education"
```

Or, equivalently:

```{r, eval = FALSE}
"SAT ~ Income 
SAT ~ Education
Income ~~ Education"
```

This syntax specifies two regression equations and one covariance. However, three more parameters are included by lavaan per default:

1. The **residual** (unexplained) variance in SAT
2. The variance of Income
3. The variance of Education

So, strictly speaking, if you don't want to rely on the default settings, the syntax would be:

```{r, eval = FALSE}
"SAT ~ Income + Education
Income ~~ Education
SAT~~SAT
Income~~Income
Education~~Education"
```
\details


### Performing the analysis

In `lavaan`, models are fit using the `sem()` function. Run the command `?sem` to open the help file for this function. Try to figure out how to take the syntax you wrote for the previous question, and fit it to the Hamilton data.

<details>
  <summary>Click for explanation</summary>

```{r}
# Load the lavaan package
library(lavaan)
# Fit the model to df, and store the result in an object called 'fit'
fit <- sem(model = "SAT ~ Income + Education
                    Income ~~ Education",
           data = df)
```
This will result in a warning about the variances. You can ignore this.
\details

### Viewing the output

Most of the relevant output of a `lavaan` analysis can be extracted using the `summary()` function. Get a summary for the analysis now. Do either of the predictors have a significant effect on SA? By specifying the option `rsquare = TRUE` in the `summary()` function, you can additionally get squared multiple correlations for the dependent variables.

<details>
  <summary>Click for explanation</summary>

```{r}
summary(fit, rsquare = TRUE)
```
\details


### Plotting the output

The package `semPlot` can automatically plot simple SEM models, like path models and CFA models. To visualize this SEM model, install the semPlot package, and use the function `semPaths`:

```{r, eval = FALSE}
install.packages("semPlot")
library(semPlot)
semPaths(fit)
```
```{r, echo = FALSE}
library(semPlot)
semPaths(fit)
```

The default plot can be improved upon, for example, by plotting the parameter estimates onto the paths, and rotating it to match our initial conceptual model at the start of this tutorial:

```{r}
semPaths(fit, whatLabels = "est", rotation = 2)
```

<!--chapter:end:07-Week3_home.Rmd-->

# Week 3 - Class

This week, we will analyze the data from the European social survey, and the paper by Kestilä for the last time. Last week, you first replicated the results by Kestilä, and then ran your own set of factor analyses. Hopefully you experienced yourself that it matters quite a bit what type of factor analysis method you choose; both for the interpretation of your factors, and analyses that incorporate factor scores. 

Instead of doing Exploratory Factor Analysis, another way of analyzing the data from the European Social Survey would be to use Confirmatory Factor Analysis. During this practical, you will conduct a CFA and compare your results to earlier EFA and PCA results and the article by Kestilä.

### Question 1

Download the ESS data from Blackboard, and load the data into R. The data are in the file *essround1-c.RData*. This is a special R-filetype that can contain several R-objects. You can load it into R using the syntax:

```{r}
load("essround1-c.RData")
```

Furthermore, you will have to load the package `lavaan` before starting with the exercises, and for convenience, copy the object `essround1_c` into an object called `data`.

<details>
  <summary>Click for explanation</summary>
```{r, warning = FALSE, message = FALSE}
library(lavaan)
data <- essround1_c
```
\details

Furthermore, we want to work with numeric variables instead of factors once again, and with the countries of interest only.

<details>
  <summary>Click for explanation</summary>
```{r}
data[c(7:44, 48, 49, 50)] <- lapply(data[c(7:44, 48, 49, 50)], as.numeric)
data$yrbrn <- as.numeric(as.character(data$yrbrn))
df <- data[data$cntry %in% c("Austria", "Belgium", "Denmark", "Finland", "Germany", "Italy", "Netherlands", "Norway", "Sweden"), ]
df$cntry <- factor(df$cntry)
```
\details

### Question 2

First, review your EFA-results for the 'trust in politics' items, as well as the question wordings of the items. How many factors do you expect?

### Question 3

Build a CFA model for the trust in politics items by means of the R-package `lavaan`. A tutorial example is available here: http://lavaan.ugent.be/tutorial/cfa.html

Make sure to ask for model fit statistics. What do you find for the value of Chi-square, df, RMSEA and CFI? Any idea why you find this Chi-square value? Does the model fit the data?

<details>
  <summary>Click for explanation</summary>
```{r, message = FALSE}
trust_model_3f <- 'trustpol  =~ pltcare + pltinvt + trstplt
                   satcntry  =~ stfeco + stfgov + stfdem + stfedu + stfhlth
                   trustinst =~ trstlgl + trstplc + trstun + trstprl'

fit_trust_model_3f <- cfa(trust_model_3f, data = df)
summary(fit_trust_model_3f, fit.measures = TRUE)
```
\details

### Question 4

As an alternative model, build a 1-factor model, with the same items as you used before, and one trust in politics factor. Evaluate the Chi-square, df, RMSEA and CFI again. Does the one factor-model fit better or worse than the factor model you previously estimated?

*Note, there is also a formal way to test whether a difference between two chi-square values is significant; more on that in the practicals of week 4.*

<details>
  <summary>Click for explanation</summary>

```{r, message = FALSE}
trust_model_1f <- 'political_trust =~ pltcare + pltinvt + trstprl + trstplt + stfeco + stfgov + stfdem + stfedu + stfhlth + trstlgl + trstplc + trstun + trstep'

fit_trust_model_1f <- sem(trust_model_1f, data = df)
summary(fit_trust_model_1f, fit.measures = TRUE)
```
\details

### Question 5

Similarly, can you think of a 2-factor model that would explain political trust? Build this model as well, and compare Chi-square, df, RMSEA and CFI to both the 1-factor and the 3-factor model. Which of the models is the best in your opinion?

By now, you should be able to perform your own two-factor CFA, based on substantive grounds. If you do not know how to do this immediately, please have a look at question 3.

**Note: None of the models fit really well. In practice, this would mean that you would have to change the model. For now, stick with the best model you have.**

### Question 6

Choose your best model, and ask for the standardized estimates by means of the addition `standardized = TRUE` in the summary command. Which item is the best predictor of the first factor?

<details>
  <summary>Click for explanation</summary>
  
```{r}
summary(fit_trust_model_3f, fit.measures = TRUE, standardized = TRUE)
standardizedsolution(fit_trust_model_3f)
```
\details


### Question 7

Byrne (2005) states that under certain conditions, a second order CFA can be specified. Would the political trust model qualify for a second order factor model?

### Question 8

Specify a second-order factor model and run this model. What do you conclude when you evaluate model fit? Is this model better than your model that you selected in question 6?

<details>
  <summary>Click for explanation</summary>
To run a second-order factor model, you can simply add an additional line within the single quotes containing the factors that you want in the second-order factor model, like in the example below.

```{r, message = FALSE}
trust_model_3f2o <- 'trustpol  =~ pltcare + pltinvt + trstprl + trstplt
                     satcntry  =~ stfeco + stfgov + stfdem + stfedu + stfhlth
                     trustinst =~ trstlgl + trstplc + trstun + trstep
                     trust     =~ trustpol + trustinst'

fit_trust_model_3f2o <- sem(trust_model_3f2o, data = df)
summary(fit_trust_model_3f2o, fit.measures = TRUE)
```
\details

### Question 9

Build a new one-factor model, only using the 3 items that ask about the respondents trust in institutions with 1) trust in the legal system, 2) trust in the police and 3) trust in the UN (see below). You can also take the full model you specified in either question 6 and question 9, but you might experience that the model becomes complicated due to the large amount of arrows.

After doing this, add as predictors of the latent factor: gender, age, education in years, political interest and self-placement on the left right scale.

![](week3class1.png)

Estimate the model. The model doesn't fit very well, but for now, we will stick with this model. Write down the regression coefficients (standardized and unstandardized) and relevant test statistics.

*Hint: Once you add predictions to your model, you should use* `sem()` *instead of* `cfa()`.

<details>
<summary>Click for explanation</summary>

We will first have to do some recoding again. You might want to make an age variable instead of a yearborn variable, which eases the interpretation. Furthermore, a dummy for gender is more informative than just the variable `gndr`, although this will be treated as a dummy. You will also have to recode the variables `eduyrs` and `lrscale` to numerical variables, and you will have to make a two-category dummy for the variable `polintr`.


```{r}
df$age <- 2002 - df$yrbrn

model_q9 <- 'trustinst =~ trstlgl + trstplc + trstun
             trustinst ~ gndr + age + eduyrs + polintr + lrscale'

fit_model_q9 <- sem(model_q9, data = df)
summary(fit_model_q9, fit.measures = TRUE, standardized = TRUE)
```

```{r, echo = FALSE, eval = FALSE}
df$age <- 2002 - as.numeric(as.character(df$yrbrn))

df$female <- as.numeric(df$gndr)
df$female <- as.numeric(df$female == 2)
df$female <- factor(df$female,
                      levels = c(0,1),
                      labels = c("Male",
                                 "Female"))

df$eduyrs <- as.numeric(df$eduyrs)
df$polintr_dummy <- as.numeric(df$polintr)
df$polintr_dummy <- as.numeric(df$polintr_dummy <= 2)
df$polintr_dummy <- factor(df$polintr_dummy,
                                   levels = c(0,1),
                                   labels = c("Not or hardly interested",
                                              "Quite or very interested"))

df$lrscale <- as.numeric(df$lrscale)

model_q9 <- 'trustinst =~ trstlgl + trstplc + trstun
             trustinst ~ female + age + eduyrs + polintr_dummy + lrscale'

fit_model_q9 <- sem(model_q9, data = df)
summary(fit_model_q9, fit.measures = TRUE, standardized = TRUE)
```
\details

### Optional 

You can plot the resulting model using `semPaths()`:

```{r}
library(semPlot)
semPaths(fit_model_q9, whatLabels = "est", rotation = 4)
```

### Question 10

Now, replace the latent score "trust in institutions" with the EFA factor score 'trust in institutions'. Delete the separate indicators, so you end up with the model below.

![](week3class2.png)


<details>
<summary>Click for explanation</summary>

```{r}
model_q10 <- 'trustinstEFA ~ gndr + age + eduyrs + polintr + lrscale'

fit_model_q10 <- sem(model_q10, data = df)
summary(fit_model_q10, fit.measures = TRUE, standardized = TRUE)
```

\details

Compare the results. Does it matter whether we use a CFA or EFA to predict trust in institutions?




<!--chapter:end:08-Week3_class.Rmd-->

# Week 4 - Home

```{r echo = FALSE, warning=FALSE, message=FALSE}
library(foreign)
data <- read.spss("SocialRejection.sav", to.data.frame = TRUE)
```
Load the data file SocialRejection.sav into R. It contains three variables: Condition (IV), SelfEst (IV), and Spent (DV).

### Question 1

Check the assumption of homogeneous regression lines (no interaction) first. What is your conclusion? 

*Hint: You need to estimate a model with, and one without an interaction, and compare them using the `anova()` function.*

<details>
  <summary>Click for explanation</summary>
```{r, warning = FALSE, message = FALSE}
ancova_main <- aov(Spent ~ Condition + SelfEst, data = data)
ancova_int <- aov(Spent ~ Condition*SelfEst, data = data)
anova(ancova_main, ancova_int)
```

The lines are homogeneous, the assumption is met (interaction is not significant).

\details


### Question 2

What should you do when this assumption is violated?

### Question 3
 
Before you can do an ANCOVA, you should also check the assumption of homogeneity. What does homogeneity imply, and is the assumption met? 

*Hint: You did this in the Week 1 class exercise.*

<details>
  <summary>Click for explanation</summary>

We can do a test of homogeneity of the variances of Spent across conditions:

```{r}
bartlett.test(formula = Spent~Condition, data = data)
```

However, note that the assumption of homogeneity actually requires the **residual** variance, after controlling for the covariate SelfEst, to be the same across conditions. We can extract these residuals from a regression with only SelfEst as a predictor:

```{r}
reg_selfest <- lm(Spent ~ SelfEst, data = data)
residuals_selfest <- reg_selfest$residuals
```

Then, we can test the null hypothesis that the error variance of the dependent variable is equal across groups:

```{r}
bartlett.test(residuals_selfest, data$Condition)
```

The test is not significant, meaning that the error variances are indeed equal, the assumption is met.

\details

### Question 4

What should you do when this assumption is violated?

<details>
  <summary>Click for explanation</summary>
You cannot really "solve" this problem in classical regression or ANCOVA, because only one parameter is estimated for the error variance. In SEM, however, you can estimate different error variance parameters for each group.
\details

### Question 5

Run the actual ANCOVA (or use previous output). What are your conclusions about the effects of the factor and the covariate?

<details>
<summary>Click for explanation</summary>

```{r}
summary(ancova_main)
```

Self esteem is significant, F (1, 55) = 29.118, p < .001, the level of self esteem of the respondent is related to the amount spent.
Condition is significant after controlling for the effect of self-esteem, F (2, 55) = 4.402, p = .017, the amount spent differs between the three conditions.

\details


### Question 6

Let's examine the differences in conditional means between the three conditions. In order to do so, we can use several approaches.

#### Approach 1: Conditional means

We can obtain the conditional means of the three groups by asking for the predicted (expected) value, based on the model, for each of the three conditions, keeping the covariate constant at 0. For this, we apply the `predict()` function to the object containing our analysis. We make a small new dataset for the values that we want predictions for:

```{r}
new_data <- data.frame(Condition = c("rejection", "neutral", "confirming"), 
                       SelfEst = c(0, 0, 0))
predict(ancova_main, new_data)
```

What are your conclusions about the three conditions (i.e., how do they differ)?

#### Approach 2: Testing significance

We can test the significance for these differences using `TukeyHSD()` again, but to get the conditional means, we need to use the residuals from a model that includes only SelfEst, which we obtained before:

```{r}
reg_selfest <- lm(Spent ~ SelfEst, data = data)
residuals_selfest <- reg_selfest$residuals
anova_conditional <- aov(residuals_selfest ~ data$Condition)
TukeyHSD(anova_conditional)
```

Respondents in the rejection condition spent more, than respondents in the neutral condition and the confirming condition. These differences are not tested on significance between two groups.

#### Approach 3: Plotting the difference

This is where R really shines: We can quickly put together a plot that shows the difference between groups, along with the raw data. We use the package `ggplot2`

```{r}
library(ggplot2)
# Put the data for the plot together
plot_data <- data.frame(Spent_resid = residuals_selfest,
                        Condition = data$Condition)
# Basic plot; indicate that you want condition on the x-axis and Spent_resid on
# the y-axis
ggplot(plot_data, aes(x = Condition, y = Spent_resid)) +
  geom_boxplot() + # Add a boxplot for each condition
  geom_jitter(width = .2) + # Plot raw datapoints on top
  theme_bw() # Add a nice APA theme
```

### Question 7

An AN(C)OVA can also be specified as a regression analysis. R automatically creates dummies. Use the `lm()` function instead of `aov()`, and compare the results.

<details>
<summary>Click for explanation</summary>
```{r}
ancova_main <- aov(Spent ~ Condition + SelfEst, data = data)
summary(ancova_main)
lm_main <- lm(Spent ~ Condition + SelfEst, data = data)
summary(lm_main)
```
\details

You can get the conditional means directly from this `lm()` model by dropping the intercept, using `-1` (which means: minus the intercept) in the formula:

```{r}
lm_no_intercept <- lm(Spent ~ -1 + Condition + SelfEst, data = data)
summary(lm_no_intercept)
```

### Question 8

To perform this analysis as a structural equation model, we need to manually compute dummy variables. We can use the function `model.matrix()` to "expand" a factor variable into dummies:

```{r}
data_dummies <- model.matrix(~ -1 + Condition, data = data)
head(data_dummies)
```

We can then bind these columns with dummies to our original data using `cbind()` (column bind):

```{r}
data <- cbind(data, data_dummies)
head(data)
```

Begin by specifying the model in lavaan like this: 

![](week4home1.png)


<details>
<summary>Click for explanation</summary>

```{r}
library(lavaan)
ancova_lavaan <- sem('Spent ~ SelfEst + Conditionrejection + Conditionconfirming', data = data)
summary(ancova_lavaan)
```

To obtain a plot of these results and compare it to our picture above, use `SemPlot`:

```{r}
library(semPlot)
semPaths(ancova_lavaan, whatLabels = "est", rotation = 2)
```
\details 

### Additional options

**Note:** When you are doing an ANCOVA (even as a regression model with dummies), you want to analyze both the covariance structure AND the mean structure. To include the latter in your analysis, you have to tell lavaan to include this by adding the argument `meanstructure = TRUE` in the fitting function:

```{r}
library(lavaan)
ancova_lavaan <- sem('Spent ~ SelfEst + Conditionrejection + Conditionconfirming', 
                     data = data,
                     meanstructure = TRUE)
summary(ancova_lavaan)
```

To obtain the standardized results and the proportion of explained variance (= squared multiple correlation, i.e., R2), you can use the options in the `summary()` function:

```{r}
summary(ancova_lavaan, standardized = TRUE, rsquare = TRUE)
```

### Question 9

Compare your results to those obtained with the regression analysis. What is your conclusion?

### Question 10 

Check the model fit. What do you conclude? 

*Note: Use* `summary()` *and* `fit.measures = TRUE`

<details>
  <summary>Click for explanation</summary>
  
```{r}
summary(ancova_lavaan, fit.measures = TRUE)
```

Saturated model, so perfect fit. Because the number of parameters to be estimated is equal to the number of observed statistics, there is a perfect fit. Here our interest is mainly in getting the estimates, not in the model-fit.
\details

<!--chapter:end:09-Week4_home.Rmd-->

# Week 4 - Class

```{r echo = FALSE, warning=FALSE, message=FALSE}
library(foreign)
library(semTools)
library(lavaan)
data <- read.spss("SocialRejection.sav", to.data.frame = TRUE)
```

You will continue with the analysis of the Social Rejection data from the Take Home exercise. Instead of running the model as a regression model with dummy variables (as in the Take Home exercise), in lavaan you can also run the model as a regression model with multiple groups. So, when you have data with categorical and continuous independent variables, as in the data file SocialRejection.sav, you could either perform:

1. an ANCOVA in R
2. a regression analysis with dummy variables (in R or lavaan)
3. or you could perform a multiple group analysis in lavaan. 

The advantages of performing a multiple groups analysis in lavaan in this situation are:

* You can allow for differences in the (residual) variances across the groups (= violation of assumption of homogeneity)
* You can more easily test the assumption of homogenous regression lines

In a multiple group analysis you specify one model and test whether this model is correct for all the groups, or whether there are differences between the groups.

### Specify basic model 

First, specify the following model as a text string, so you can later use it in lavaan:

![](week4class1.png)

<details>
  <summary>Click for explanation</summary>
```{r}
reg_model <- "Spent ~ SelfEst"
```
\details

### How to run multi-group model

To run this model as a multi-group model, you can specify the argument `group =` when running the analysis:

```{r}
library(lavaan)
multi_group <- sem(reg_model,
                   data = data, 
                   group = "Condition")
```

Now, obtain the standardized estimates and the squared multiple correlations (r square) for this model.

<details>
  <summary>Click for explanation</summary>
```{r}
summary(multi_group, standardize = TRUE, rsquare = TRUE)
```
\details

### Question 1

Report on the parameter estimates for the three groups; i.e., the regression coefficients, the intercepts (marginal means of Spent), and the residual variances (i.e., the variances of the residuals of Spent).

**Note:** What's missing from the model are the means and variances of SelfEst. That's because any variable that is **purely independent** is not strictly considered to be part of the model. We can manually incorporate it into the model by estimating the intercept of SelfEst. This will make SelfEst part of the model, so we will also get its variance. You estimate the intercept of a variable by adding this code to your model:

```{r, eval = FALSE}
"SelfEst ~1"
```

### Question 2

Which constraints would we need to impose, across the groups, in order to make this multi-group model equivalent to an ANCOVA?

<details>
  <summary>Click for explanation</summary>
1.	Fix regression lines of the covariate to be equal across groups
2.	Fix residual variances to be equal across groups
\details

### Question 3
 
An ANCOVA tests whether the means of several groups on the DV (Spent) are all equal (while controlling for the covariate). What are the $H_0$ and $H_1$ of an ANCOVA?

<details>
  <summary>Click for explanation</summary>
$H_0$: The intercepts of Spent are all equal across groups
$H_1$: There is a difference in intercepts of Spent across groups
\details

### Question 4

You could test the null-hypothesis that you formulated in the previous question by imposing one more constraint across the three groups. What is this constraint?

<details>
  <summary>Click for explanation</summary>
Fix all intercepts to be equal.
\details

### Imposing constraints

So, by putting constraints to these models, we can make the same model as an ANCOVA. In lavaan, we impose constraints by **giving labels** to parameters. You use `c()` to make a vector of labels that is equally long as the number of groups, and you can give any name to the labels. You then use the `*` symbol to assign it to a parameter. The syntax looks like this:

```{r, eval = FALSE}
"Spent ~ c(labelgroup1, labelgroup2, labelgroup3) * SelfEst"
```

If you use the same name multiple times, these parameters will be constrained to be equal:

```{r, eval = FALSE}
"Spent ~ c(label, label, label) * SelfEst"
```

Constraints for variances are specified similarly:

```{r, eval = FALSE}
"Spent ~~ c(label, label, label) * Spent"
```

### Stepwise approach

If you want to add several restrictions (contraints), we can impose them all at once, or in a stepwise manner. Here, we will explain the stepwise approach. But if you know which model you want to run (e.g., an ANCOVA), you can also skip these steps and just run your final model and check if the fit is good.

First you test the model without the constraints and the next step is that you test the model with the first constraint, then a model with the first and second constraint, and so forth. These are all nested models. Give the models informative names, so you can easily compare them.

**NOTE:** make sure to specify increasingly more restricted models, that is, do not release restrictions once they have been imposed). So you go from completely free to most restrictive. You can compare subsequent models using chi-square difference tests (ironically, by calling the `anova()` function) to ensure that the constraints you imposed are tenable. Note that if the chi-square difference test is significant, this means you CANNOT impose the constraint!

In this case, we could run the following nested models: 

*Unconstrained model.* No constraints are imposed

*Model 1.* Structural weights: constrain the regression coefficient across groups

*Model 2.* Structural residuals: constrain the variances and covariances of the residuals of the endogenous variables (here: Spent)

*Model 3.* Structural intercepts: constrain the intercepts of the endogenous variables across groups

We can compare two of these models using the `anova()` function:

```{r, eval =F}
anova(m1, m2)
```

However, with more than two models, it is convenient to compare all of them at once. For this, we can use the function `compareFit()` from the  `semTools` package (which you have to install):

```{r, eval = F}
library(semTools)
compareFit(weights = m1,
           residuals = m2,
           intercepts = m3)
```

### Question 5

Run the series of models (nested models) as described above. Compare the models and report the chi-square difference test of this comparison. Why does this test have the df it has? What is your conclusion about the constraints you imposed? What does this mean?

<details>
  <summary>Click for explanation</summary>
```{r}
mu <- sem("Spent ~ SelfEst",
          data = data, 
          group = "Condition")

m1 <- sem("Spent ~ c(a, a, a) * SelfEst",
          data = data, 
          group = "Condition")
m2 <- sem("Spent ~ c(a, a, a) * SelfEst
          Spent ~~ c(b, b, b) * Spent",
          data = data, 
          group = "Condition")
m3 <- sem("Spent ~ c(a, a, a) * SelfEst
          Spent ~~ c(b, b, b) * Spent
          Spent ~ c(c, c, c) * 1 ",
          data = data, 
          group = "Condition")

compareFit(unconstrained = mu,
           regression = m1,
           residuals = m2, 
           intercepts = m3)
```

In model 1 we constrained the regression lines to be equal across groups. In this model there is 1 regression coefficient estimated instead of 3, so the difference in df = 3-1=2. The associated chi-square is not significant Chi2(2) = 0.74, p = .69. This means that the regression lines can be considered equal across groups, and the first assumption of Ancova holds. 

In model 2 we additionally constrained the residual variance of Spent to be equal across groups. In this model there is 1 residual variance estimated instead of 3, so the difference in df with the previous model is 3-1=2. The associated chi-square is not significant Chi2(2) = 1.74, p = .61. This means that the residual variances of Spent can als be considered equal across groups, and both assumptions hold. 
\details

### Question 6

Next, compare Model 3 to Model 2. Report the chi-square difference test, what is the conclusion now? Is it the same conclusion from the ANCOVA? Report the relevant results for both the ANCOVA and the Multi-group model, compare the results. (Also think about: what would you report in an article?)

<details>
  <summary>Click for explanation</summary>
Additionally constraining the intercepts to be equal across groups significantly deteriorated model fit, Chi2(2) = 7.61, p = .02. This means that the intercepts cannot be considered equal across groups. There is thus a significant difference in Spent between these conditions, controlled for Self-Esteem. In model 2 we see that, compared to the neutral condition (M = 10.27), the intercept is highest in the Rejection condition (M = 12.45) and lowest in the Confirming condition (M = 9.15). 

In the ANCOVA in SPSS we saw that the effect of Condition was significant after controlling for the effect of self-esteem, F (2, 55) = 3.784, p < .05, so the amount spent differs between the three conditions. Respondents in the rejection condition spent more, than respondents in the neutral condition and the confirming condition.

The two conclusions are similar. 
\details

<!--chapter:end:10-Week4_class.Rmd-->

# Week 5 - Home

```{r echo = FALSE, warning=FALSE, message=FALSE}
library(foreign)
library(semTools)
library(lavaan)
data <- read.spss("SelfEsteem.sav", to.data.frame = TRUE)
```

The data file SelfEsteem.sav contains the following variables: 

* Parental Attachment
* Peer Attachment
* Empathy
* Prosocial behavior
* Aggression
* Self-esteem

Which were measured for 143 college students (mean age: 18.6 years, SD=1.61). The researcher is interested in the direct and indirect effects of parental and peer attachment on self-esteem, and the mediating roles of empathy and social behavior (i.e., prosocial behavior and aggression).

Specifically, the researcher expects that having good relationships with peers will increase prosocial behavior and decrease aggressive behavior, and that this is probably mediated through empathy (since people who experience empathy are likely to want to decrease distress in others), while the relationships with parents are expected to have a more direct effect on self-esteem.

### Specify model

To investigate these research questions, you need to specify the model depicted below. Create a text string describing this model for later use with lavaan:

![](week5home.png)

<details>
  <summary>Click for explanation</summary>
```{r}
mediation_model <- "SelfEst ~ ProSoc + Aggr + ParAtt + PeerAtt
                    ProSoc ~ Emp
                    Aggr ~ Emp
                    Emp ~ ParAtt + PeerAtt
                    ProSoc ~~ Aggr
                    ParAtt ~~ PeerAtt"
```
\details

### Question 1

How many and which paths are there from Parental Attachment to Self-esteem? And from Peer Attachment to Self-esteem? 

<details>
  <summary>Click for explanation</summary>
PARA->SE: 3 if you dont count covariances
	       6 if you count covariance between PARATT and PEERATT
	       10 if you (additionally) count the covariance between e2 and e3
I think the indirect pathway in AMOS does not count covariances, so the indirect effect estimate in AMOS is made up of PARATT -> EMP -> PROSOC -> SELFEST, and of PARATT -> EMP -> AGGR -> SELFEST

PEERA->SE: as above
\details

### Question 2

Run this model. Discuss the fit of the model.

<details>
  <summary>Click for explanation</summary>
```{r}
fit_mediation <- sem(mediation_model,
                     data = data)
summary(fit_mediation, fit.measures = TRUE)
```

CFI is acceptable, RMSEA is not.
\details

### Question 3

Considering the parameter estimates, what can you say about the research questions? 


<details>
  <summary>Click for explanation</summary>
The researcher expects that having good relationships with peers will increase prosocial behavior and decrease aggressive behavior, and that this is probably mediated through empathy (since people who experience empathy are likely to want to decrease distress in others), while the relationships with parents are expected to have a more direct effect on self-esteem.

Generally seems to be the case – Paratt has a significant direct effect, and ParAtt seems to have a significant effect on empathy. Need to look further at indirect effects and total effects for details.
\details

### Estimating indirect effects

Remember that an indirect effect is the product of several chained direct effects. Therefore, to obtain an indirect effect, you can multiply the two (or more) direct effects it consists of. Based on this knowledge - and you previous experience labelling parameters - the way to estimate indirect effects may be obvious:

1. You label the direct effects
2. You multiply the labels

There is a tutorial on this on the lavaan website: http://lavaan.ugent.be/tutorial/mediation.html

Additionally, a total effect is the sum of all direct and indirect effects that connect one predictor and one outcome. So, for example, in this week's model, two indirect effects link Empathy and Self-esteem (through prosocial and aggression). The total effect of empathy on self-esteem is therefore the sum of these two indirect effects (and no direct effect).

### Question 4

Estimate the indirect effects of Emp on SelfEst, mediated through ProSoc and Aggr. Also estimate the total effect.

*Note: A new parameter is defined in lavaan using the* `:=` *operator.*

<details>
  <summary>Click for explanation</summary>
```{r}
mediation_model <- "SelfEst ~ bse_pro * ProSoc + bse_agg * Aggr + ParAtt + PeerAtt
                    ProSoc ~ bpro_emp * Emp
                    Aggr ~ bagg_emp * Emp
                    Emp ~ ParAtt + PeerAtt
                    ProSoc ~~ Aggr
                    ParAtt ~~ PeerAtt
                    ind_pro := bse_pro * bpro_emp
                    ind_agg := bse_agg * bagg_emp
                    total := ind_pro + ind_agg"
fit_mediation <- sem(mediation_model,
                     data = data)
summary(fit_mediation, fit.measures = TRUE)
```
\details

### Question 5

Estimate all indirect effects and the total effects of Parental Attachment and Peer Attachment on Self-esteem.

<details>
  <summary>Click for explanation</summary>
```{r}
mediation_model <- "SelfEst ~ bse_pro * ProSoc + bse_agg * Aggr + bse_par * ParAtt + bse_peer * PeerAtt
                    ProSoc ~ bpro_emp * Emp
                    Aggr ~ bagg_emp * Emp
                    Emp ~ bemp_par * ParAtt + bemp_peer * PeerAtt
                    ProSoc ~~ Aggr
                    ParAtt ~~ PeerAtt
                    ind_par_emp_pro := bse_pro * bpro_emp * bemp_par
                    ind_peer_emp_pro := bse_pro * bpro_emp * bemp_peer
                    ind_par_emp_agg := bse_agg * bagg_emp * bemp_par
                    ind_peer_emp_agg := bse_agg * bagg_emp * bemp_peer
                    
                    total_par := bse_par + ind_par_emp_pro + ind_par_emp_agg
                    total_peer := bse_peer + ind_peer_emp_pro + ind_peer_emp_agg"
fit_mediation <- sem(mediation_model,
                     data = data)
summary(fit_mediation)
```

\details

### Question 6

To compare the total effect of parental attachment versus peer attachment on self esteem, should you use the standardized or unstandardized parameters? Obtain the appropriate output, and draw a conclusion.

<details>
  <summary>Click for explanation</summary>

You should use the standardized results:

```{r}
summary(fit_mediation, fit.measures = TRUE, standardize = TRUE)
```
\details

### Difference between parameters

To test the difference between parameters, you can constrain them to be equal and do a chi-square difference test to compare the constrained and unconstrained models. However, you can also calculate the difference between parameters, and test if it's significant:

```{r, eval = F}
"dif_par_peer := total_par - total_peer"
```

### Bootstrapping

You may have learned before that the sampling distribution of indirect effects is not normal. Consequently, you cannot use parameteric p-values to determine the significance of indirect effects because they are biased. If you want to know whether the indirect effects are significant, you can bootstrap them to obtain a 95% confidence interval.

Bootstrapping means that lavaan will draw 1000 samples from the data and estimate the model on each sample. There will thus be 1000 estimates for every parameter. The lower and upper bounds of a 95% confidence interval are determined by taking the 2.5% and 97.5% quantiles of the 1000 samples for each parameter.

The confidence interval is interpreted the same as a normal confidence interval: If zero lies inside the interval (e.g., lower bound is -.4 and upper bound is .9), we conclude that the parameter is not significantly different from zero, but if zero does not lie in this interval (e.g., lower bound is .4 and upper bound is 1.3), we can say the parameter differs from zero (at an alpha of .05 since we are considering a 95% confidence interval). 

We will get these intervals for all the parameters in the model, but we are specifically interested in the intervals for the indirect effects, because we want to know if they differ from zero (i.e., whether there is a mediated effect).

In lavaan, bootstrap standard errors are requested by specifying se = "bootstrap" in the fitting function, and specifying the number of bootstrap samples as `bootstrap = 1000`. First, just get your code running by specifying a low number, like 100 or even 10. **Note: To draw reliable conclusions from the results, you should always use 1000+ bootstrap samples - but it can take a long time.**

```{r}
fit_mediation <- sem(mediation_model,
                     data = data,
                     se = "bootstrap",
                     bootstrap = 100)
```

To obtain the confidence intervals for your model, use the following syntax:

```{r, warning=FALSE}
parameterestimates(fit_mediation, boot.ci.type = "bca.simple", standardized = TRUE)
```

### Question 7 

What do you conclude about the indirect and total effects of Parental attachment and Peer attachment on Self-esteem? Report your conclusions as you would report them in a paper (in words and statistics).

<!--chapter:end:11-Week5_home.Rmd-->

# Week 5 - Class

```{r echo = FALSE, warning=FALSE, message=FALSE}
library(foreign)
library(semTools)
library(kableExtra)
library(lavaan)
library(psych)
data <- read.spss("WorkingMom_2014.sav", to.data.frame = TRUE)
```

The effect of professional child care on the cognitive, emotional, and social development of children is a much debated toping among policy makers as well as parents with young children. In particular the effect of child care during the first year of life has been of interest.

In the current study the researchers are interested in the relationship between mothers' work status during the first year of life and their children's cognitive development by age 4.5 years, measured with the Woodcock Johnson Achievement and Cognitive Batteries (WCJ). Other variables that are considered of interest and which were included are: Mother’s earnings at age 4.5; Home environment quality; Mother’s sensitivity; and Mother’s depression.

Mother's work status has three levels: 

1. full time working moms
2. part time working moms, and
3. stay-at-home moms.

### The analysis

If you were to analyze these data, you could do an ANCOVA analysis, with Work (mother's working status) as factor, and the other independent variables as covariates. Here you find the results of the ANCOVA analysis (assumptions of the ANCOVA were checked and okay). 

```{r echo = FALSE, echo = FALSE}
fit <- lm(WCJ ~ Work + MothEar + HomeEnv + MothSen + MothDep, data)
print(anova(fit))
```

These are the parameter estimates for that model:

```{r echo = FALSE, eval = TRUE}
print(summary(fit))
```


### Question 1

Based on the results, what would be your advise for women with children under one year of age, and why?

### Dummy coding

Consider the other variables that were included. We may expect that these are actually influenced by mother's work status. For instance, mother’s earnings by age 4.5 may depend on whether or not the mother was working during the first year, and also on whether she was working full time or part time. This implies there may be *mediated* or indirect effects of work status on the outcome variable. 

To investigate this, we can specify a SEM model in which the effects of work status are mediated through the four other variables. Note that work status is a categorical (ordinal) variable, with three categories. To include this variable as a predictor, we need to dummy-code it. `lm()` does this automatically; `lavaan` doesn't. The function `dummy.code` in `library(psych)` can help us:

```{r, eval = FALSE, message=FALSE}
library(psych)
dummy.code(data$Work)
```
```{r, echo = FALSE}
head(psych::dummy.code(data$Work))
```

We need to add these variables to our `data`. Also, the names of the dummies include spaces, so we should rename them. One easy way to do this is to change the levels of the original variable. Then, we add the dummies to `data`:

```{r}
levels(data$Work)
levels(data$Work) <- c("workfull", "workpart", "workno")
data <- data.frame(data, dummy.code(data$Work))
```

If we want to use the stay-at-home moms as the reference group, we just use the other two dummies as predictors. Additionally, if we want to know the intercept of `WCJ` for the reference category, we need to add the argument `meanstructure = TRUE` to the `sem()` function call.

### Question 2

We now want to specify a SEM model in which the effects of work status are mediated through the four other variables. What is the syntax of this model?

<!--Note: If two variables are purely independent in your model (i.e. they have no arrows drawn towards them), a covariance arrow between them needs to be drawn. Otherwise the model will be fit as if their covariance is 0. (Which is usually not the case, the dummy variables do covary.)-->


<details>
  <summary>Click for explanation</summary>
```{r}
model <- '
WCJ ~ MothEar + HomeEnv + MothSen + MothDep + workfull + workpart
MothEar ~ workfull + workpart
HomeEnv  ~ workfull + workpart
MothSen  ~ workfull + workpart
MothDep  ~ workfull + workpart 
'
```

\details

### Question 3

If you had no access to lavaan, what regression analyses could you run in order to piece together this mediated path model? What else would we need to do?

<details>
  <summary>Click for explanation</summary>
You would estimate these regression analyses:
```{r, eval = FALSE}
fit_y <- lm(WCJ ~ MothEar + HomeEnv + MothSen + MothDep + workfull + workpart, data)
fit_m1 <- lm(MothEar ~ workfull + workpart, data)
fit_m2 <- lm(HomeEnv  ~ workfull + workpart, data)
fit_m3 <- lm(MothSen  ~ workfull + workpart, data)
fit_m4 <- lm(MothDep  ~ workfull + workpart, data)
```

Additionally, we would have to multiply the effects of the dummies on the mediators, with the effects of the mediators on WCJ, in order to calculate the indirect effects.
\details


### Question 4

Add syntax to compute the indirect effects to your model from question 2, and run the analysis. Apply an appropriate solution to test whether the indirect effects are significant or not. Discuss the indirect effects of work status on the outcome variable. Taking the direct effects into account, can you explain the differences between the two dummy variables?

<details>
  <summary>Click for explanation</summary>
First, label all regressions, and add syntax for the indirect effects:

```{r}
model <- '
WCJ ~ ear * MothEar + env * HomeEnv + sen * MothSen + dep * MothDep + workfull + workpart
MothEar ~ earfull * workfull + earpart * workpart
HomeEnv  ~ envfull * workfull + envpart * workpart
MothSen  ~ senfull * workfull + senpart * workpart
MothDep  ~ depfull * workfull + deppart * workpart

i_earfull := ear * earfull
i_envfull := env * envfull
i_senfull := sen * senfull
i_depfull := dep * depfull

i_earpart := ear * earpart
i_envpart := env * envpart
i_senpart := sen * senpart
i_deppart := dep * deppart
'
```

Then, estimate the model. Specify bootstrapped SEs and 1000 bootstrap samples. Remember that, to get the intercepts of dependent variables for the reference category, we should specify `meanstructure = TRUE`:

```{r, message = FALSE, eval=FALSE}
library(lavaan)
fit <- sem(model, 
           data, 
           meanstructure = TRUE,
           se = "bootstrap", 
           bootstrap = 1000)
```
```{r, message = FALSE, echo=FALSE}
#save(fit, file = "boot_sem5class.RData")
load("boot_sem5class.RData")
```

To obtain the 95% CIs for the indirect effects, we can use the following syntax. First, I obtain the `parameterestimates()` and put them in an object called `pars`. Then, I ask for only the **rows** that contain my defined parameters (`pars$op == ":="`), and only the columns `c("label", "est", "ci.upper", "ci.lower")`:

```{r, eval = FALSE}
pars <- parameterestimates(fit, boot.ci.type = "bca.simple")
pars[pars$op == ":=", c("label", "est", "ci.upper", "ci.lower")]
```
```{r, echo = FALSE}
pars <- parameterestimates(fit, boot.ci.type = "bca.simple")
print(pars[pars$op == ":=", c("label", "est", "ci.upper", "ci.lower")], row.names = FALSE)
```
\details


### Question 5

If we want to determine whether working or not is harmful for children’s cognitive development, what should we consider: the direct effects, the indirect effects, or the total effects, and why? 

### Question 6

Compare the effect of the dummy variables working full time and working part time in a regression model with all control variables, to the parameters in the lavaan model. Which parameter estimates from the regression correspond to your lavaan model, and why? 

<details>
  <summary>Click for explanation</summary>
First, run the requested regression:

```{r}
fit_reg <- lm(WCJ ~ workfull + workpart + MothEar + HomeEnv + MothSen + MothDep, data)
summary(fit_reg)
```

These coefficients are all identical to the direct effects on WCJ from your lavaan model.

The only difference is that your lavaan model ADDITIONALLY estimates the effects of the dummies on the mediators, AND computes the indirect effects.
\details


### Question 7

What are your most important conclusions about the effect of working status on cognitive development? You can use your answers to the previous questions. But a model with many variables such as this one has many possible interesting options to look at. Just look around for any interesting results! 

<!--chapter:end:12-Week5_class.Rmd-->

# Week 6 - Home

```{r echo = FALSE, warning=FALSE, message=FALSE}
library(foreign)
library(semTools)
library(kableExtra)
library(lavaan)
library(psych)
data <- read.spss("suiciderisk.sav", to.data.frame = TRUE)
```

Exercise based on: 

Metha, A., Chen, E, Mulvenon, S. and Dode, I. (1998). A Theoretical Model of Suicide Risk. *Archives of Suicide Research, 4*, p.  115-133.

Download the dataset “suiciderisk.sav” from blackboard. This is a dataset that was simulated from the covariance matrices in the original paper presented on p.123. The idea behind simulation is that you can recreate the dataset from some summary statistics. The covariance matrix suffices for doing this, as it summarizes all relations between all variables (see http://en.wikipedia.org/wiki/Computer_simulation for an introduction).
The new data matrix almost exactly corresponds to the data as presented in the paper, albeit that there are inconsistent data cells (negative- or our of range values for the scale of the original variables). You don’t have to worry about this now.

In the take home- and class exercise we will investigate the dataset on suicidesisk and test whether there is a possible moderation of gender.

### Question 1

Baron and Kenny (1986) present four cases to illustrate how moderation can be studied. If gender is our moderator, and either depression, hopelessness, selfesteem and/or substance abuse our independent variables, what case should we use to investigate moderation?

<details>
  <summary>Click for explanation</summary>
Case 2: dichotomous moderator, continuous independent variables
\details

### Question 2

First, run bivariate correlations of all continuous independent variables with suicide risk. Do you think it is useful to investigate these predictors? What if one one the correlations is non-significant. Is it then still useful to study moderation?

*Note: You can obtain correlations using* `cor()` *or* `psych::corr.test()`

<details>
  <summary>Click for explanation</summary>
A basic correlation matrix for the first 5 variables:

```{r}
cor(data[, 1:5])
```

A correlation table with p-values can be obtained using the `psych::corr.test()` function:

```{r}
library(psych)
corr.test(data[, 1:5])
```
Yes, correlations are similar to those in paper, and are all significant. Even when correlations are n.s., it is still useful to investigate moderation: there might be a suppression effect
\details
 
### Question 3

Examine the relationships between suicide risk and the four continuous predictors visually. What do you think about the relations?

<details>
  <summary>Click for explanation</summary>

#### A psych solution

The package `psych' contains a function to help us visually check assumptions, such as linearity:

```{r}
pairs.panels(data[, 1:5], 
             density = TRUE, # show density plots
             ellipses = TRUE # show correlation ellipses
             )
```

#### Ggplot to build plots

In this case, psych is easier. But generally, we can make any plot we want using `ggplot2`. In ggplot, we add one plot element at a time. Here is an example for one variable:

```{r}
library(ggplot2)
ggplot(data, aes(x = subabuse, y = suirisk)) +
  geom_point() +  # Add scatter
  geom_smooth() + # Add smooth trend
  theme_bw()      # Add black/white theme
```

You could also add a linear trend line:

```{r}
ggplot(data, aes(x = subabuse, y = suirisk)) +
  geom_point() +  # Add scatter
  geom_smooth() + # Add smooth trend
  theme_bw() +    # Add black/white theme
  # Add linear trend line (without confidence bound and red in color)
  geom_smooth(method = "lm", color = "red", se = FALSE)
```

This seems linear; you can check the other variables!
\details
 	 
### Question 4

Build the scatterplots again, but now map participants' gender to the colour of the dots and smooth line. Do your conclusions differ from question 3?

<details>
  <summary>Click for explanation</summary>

#### A psych solution

Again, `psych` has a standard solution for this situation:

```{r}
pairs.panels(data[, 1:5], bg = c("red", "blue")[data$gender], pch = 21)
```        

In ggplot, we can make the plots more detailed. We can map gender to the color of the dots and smooth line using `aes(colour = gender)`:

```{r}
ggplot(data, aes(x = subabuse, y = suirisk, colour = gender)) +
  geom_point() +  # Add scatter
  geom_smooth() + # Add smooth trend
  theme_bw()      # Add black/white theme
```

This seems linear; you can check the other variables! There are also some differences by gender, in range and steepness.
\details


### Question 5

Baron and Kenny (1986) note the correlational method (as we just used) has two serious deficiencies. We can investigate only one of them. Which? Please investigate whether this potential problem exists for the moderator gender.

*Note: Use* `psych::describeBy()` *or* `car::leveneTest`

<details>
  <summary>Click for explanation</summary>

The variances might not be equal for all levels of gender. We can do a preliminary investigation using `describeBy`; a version of the `describe` that splits the data by group.

```{r, eval = FALSE}
library(psych)
describeBy(data[, 1:5], group = data$gender)
```
```{r, echo = FALSE}
library(psych)
print(describeBy(data[, 1:5], group = data$gender))
```

The variances turn out to be roughly the same for most variables, but there is a large difference in the variance of nartotic substance use. Let's apply Levene's test, which lives in the `car` package:

```{r}
library(car)
leveneTest(y = data$subabuse, group = data$gender)
```

If we want a Levene's test for all five continuous variables, it can be helpful to **apply** the function to the 5 columns of data. The function `lapply()` applies a second function (`leveneTest()`) to every element of a list; in this case - the five columns. The argument `group` stays the same for each variable:

```{r}
lapply(data[1:5], leveneTest, group = data$gender)
```
\details

### Question 6

Baron and Kenny also state how this problem might be resolved by looking at regression coefficients. Run a multiple regression model, with suicide risk as dependent variable, and all 4 independent variables as predictors. Run this model separately for both genders. Do you find any differences in the regression coefficients? (unstandardized)

<details>
  <summary>Click for explanation</summary>
```{r}
 # In the lm() function, ~ . means: use all predictors
summary(lm(suirisk ~ ., data = data[data$gender == "females", 1:5]))
summary(lm(suirisk ~ ., data = data[data$gender == "males", 1:5]))
```
A small difference for substance use (.12 males, .06 females) and depression (.05 females, .115 males)? Other than that, they are similar.

\details

### Question 7

What do you think, does it make sense to study the moderating role of gender in a model that explains suicide risk?

<details>
  <summary>Click for explanation</summary>
Yes, all assumptions hold, so fine to do it.
\details

### Question 8

One way to study moderation (and this is something you might have done in an earlier statistics course), is to compute interaction terms of the moderator variable with the independent variables. R does this automatically, if you use the multiplication symbol `*` in your regression equation. 

Run a hierarchical regression analysis: Build two regression models, and include all independent variables and gender in the first model, and add the interaction terms in the second model. Then, compare the fit of the two models using `anova()` Do you find that the interaction variables explain part of the variance in suicide risk over and above that of the independent variables?


<details>
  <summary>Click for explanation</summary>
```{r}
reg_1 <- lm(suirisk ~ ., data)
summary(reg_1)
reg_2 <- lm(suirisk ~ gender * subabuse + gender * hopeless + gender * selfesteem + gender * depression, data)
summary(reg_2)

anova(reg_1, reg_2)
```
R2 goes from .20 to .22, so small difference. This difference is significant. The only significant interaction effect is gendermales:depression.

\details

### Question 9

Build a basic regression model in lavaan, see the figure below. You have built this model before. Compare your lavaan results to your earlier findings. What do you conclude?

![](week6home1.png)

<details>
  <summary>Click for explanation</summary>
  
```{r}
library(lavaan)
model <- "suirisk ~ subabuse + hopeless + selfesteem + depression + gender"
fit <- sem(model, data)
summary(fit)

library(semPlot)
semPaths(fit, whatLabels = "est", rotation = 2)
```

Same results, except for a small difference in the standard errors, that results from the fact we use Ordinary Least Squares (OLS)-estimation in R and Maximum Likelihood (ML) in lavaan. Apart from that, we can also estimate the correlations/covariances between the independent variables in lavaan, that we cannot estimate in R.
\details 

### Question 10

Do you have any idea why we find a Chi-square value of exactly 0.0?

<details>
  <summary>Click for explanation</summary>
There are no degrees of freedom (the model is identical to the saturated model) left. This means we want to estimate as many paths as there are sample moments. So, all covariances between variables are accounted for in the model, and there is no room for any deviation between our “model” and the data.
\details

### Question 11

Mehta et al (1998) state that the theory of suicide risk is more complicated than we so far modeled using regression models. The effects of self-esteem and depression are mediated through hopelessness, while depression also explains self-esteem. On page 117-118 they summarize their expectations about the effects in 6 hypotheses (H1 and H2a-e). Build a mediation model in lavaan based on these hypotheses. Leave gender out of the model for now. 

Run the model, and see whether the model fits by looking at the Chi-square(df), it’s p-value, CFI and RMSEA. In case your model does not fit, can you make it fit the data by making an alteration to the model? Note that your Chi-square values might differ slightly from those presented by Mehta et al (1998), because of data simulation procedures.

<details>
  <summary>Click for explanation</summary>
The model fits when a covariance is added between substance use and depression (see figure 1 in the article). Then, Chi-square(3) =9.4, p=.025 CFI=.992, RMSEA=.064. This is probably what Mehta et al. (1998) did as well, without showing us.
  
```{r}
model <- "
suirisk ~ hopeless + depression + subabuse
hopeless ~ depression + selfesteem
selfesteem ~ depression
subabuse ~~ depression
"

fit <- sem(model, data)
summary(fit, fit.measures = TRUE)
semPaths(fit, whatLabels = "est", rotation = 2)
```
\details

<!--chapter:end:13-Week6_home.Rmd-->

# Week 6 - Class

```{r echo = FALSE, warning=FALSE, message=FALSE}
library(foreign)
library(semTools)
library(kableExtra)
library(lavaan)
library(psych)
data <- read.spss("suiciderisk.sav", to.data.frame = TRUE)
```

Start with your last model from Question 11 of the THE. 

### Question 1

We can modify this general model for all cases into a multi-group model
with two groups (males and females). Why is this a good method to study moderation? (or in other words, what are the two research questions we can investigate with a multi-group model?)

<details>
  <summary>Click for explanation</summary>
Because we can investigate 1) whether the model itself is different for boys and girls and 2) whether the size of regression coefficients differ
\details

### Question 2

Estimate a multi-group model, with gender as grouping variable. In case you forgot how to do this, see the first of the class exercise from week 4. Look at the fit of the model, what do you find?

<details>
  <summary>Click for explanation</summary>
```{r}
model <- "
suirisk ~ hopeless + depression + subabuse
hopeless ~ depression + selfesteem
selfesteem ~ depression
subabuse ~~ depression
"

fit <- sem(model, data, 
                   group = "gender")
summary(fit, fit.measures = TRUE)
```

\details

### Question 3

Mehta et al. (1998) state that their model can be improved post-hoc by adding and removing a path to this model. Follow their procedure, and first add a path for both males and females, and secondly, remove a nonsignificant path.

<details>
  <summary>Click for explanation</summary>
```{r}
model_exploratory <- "
suirisk ~ depression + subabuse
hopeless ~ depression + selfesteem
selfesteem ~ depression
subabuse ~~ depression + hopeless
"

fit_exploratory <- sem(model_exploratory, data, 
                   group = "gender")
summary(fit_exploratory, fit.measures = TRUE)
```
\details

### Question 4

Evaluate the path coefficients of both males and females (tip: look at both the unstandardized and standardized coefficients). Can you explain how the two groups differ?

<details>
  <summary>Click for explanation</summary>
```{r}
summary(fit, fit.measures = FALSE, standardized = TRUE)
```
  

\details

### Question 5

We can test the difference between males and females more formally in two ways: 

1. By constraining the size of the regression coefficients to be equal in both groups and doing a test for nested models/
2. By computing (`:=`) a parameter for the difference between the two groups, and looking at its p-value, or a bootstrapped confidence interval. 

Why are these approaches both preferrable over just comparing regression coefficients by sight?

<details>
  <summary>Click for explanation</summary>
Even if we observe differences, we do not know whether they are significantly different. By constraining parameters to be equal, we can test two models. 1) the free model against 2) the constrained model. This is done using a Chi-square difference test. By computing a difference parameter, we can do a parameteric test or bootstrap confidence interval for the difference.
\details

### Question 6

Constrain the regression coefficients for males and females. Compare the unconstrained model to the model with constrained regression coefficients. What is your conclusion?

<details>
  <summary>Click for explanation</summary>
First, estimate the constrained model. We can use the `model` from the first exercise:

```{r}
fit_fix_reg <- sem(model, data,
                   group = "gender",
                   group.equal = "regressions")
summary(fit_fix_reg, fit.measures = TRUE)
```

Then, compare the two models. I like to use `semTools::compareFit()`:


```{r}
library(semTools)
compareFit(Free = fit,
           Constrained = fit_fix_reg)
```
  
The model gets significantly worse. This means the regression coefficients for males and females are not equal.
\details


### Specific differences

Just knowing that regression coefficients differ, is interesting in itself. Reflect here on the conclusion of Mehta et al. 1998. Do they test for significant moderation?

After doing this omnibus (overall) test, it is interesting to know which parameters, speciffically, differ. We can do this by computing new parameters for the difference between men and women. These new parameters will be tested using Z-tests and corresponding p-values. For a non-parametric test, you will have to bootstrap your analysis (see the explanation about bootstrapping indirect effects).

We use a similar approach to the one we used to compute indirect effects:

1. Label every path in your model
2. Define new parameters as the difference between corresponding parameters for men and women 

```{r}
model_diff <- "
suirisk ~ c(m1, f1)*hopeless + c(m2, f2)*depression + c(m3, f3)*subabuse
hopeless ~ c(m4, f4)*depression + c(m5, f5)*selfesteem
selfesteem ~ c(m6, f6)*depression
subabuse ~~ depression

D1 := m1-f1
D2 := m2-f2
D3 := m3-f3
D4 := m4-f4
D5 := m5-f5
D6 := m6-f6
"
```

### Question 7

Fit this model, and inspect the results for the defined parameters. What are your conclusions?

<details>
  <summary>Click for explanation</summary>
```{r}
fit_dif <- sem(model_diff, data, 
                   group = "gender")
summary(fit_dif)
```

Only the effect of depression on suicide risk, and the effect of depression on selfesteem, are significantly different between the sexes.
\details

### Question 8

Is there anything we should consider when inspecting these p-values?

<details>
  <summary>Click for explanation</summary>
You should consider the potential risk of multiple testing, and whether the assumption of normality holds.
\details

### Results table

When you want to include your results in a paper, it's a lot of work to copy-paste everything. There are many ways to get R results directly into a paper, including writing the entire paper in R and automatically updating the results. I will show you a very basic way to make a table and export it to a spreadsheet. We will use the functions `parameterEstimates(fit, standardized = TRUE)` to get the unstandardized and standardized estimates, and then put them into a nice table:

```{r}
table_results <- parameterEstimates(fit, standardized = TRUE)
head(table_results)
```

Then, we take only the labeled parameters (which are the regression coefficients and difference parameters):

```{r}
table_results <- table_results[table_results$label != "", ]
table_results <- cbind(table_results[1:6, 1:3],
               Est_men = table_results[1:6, "std.all"],
               Est_women = table_results[7:12, "std.all"],
               p_diff = table_results[13:18, "pvalue"])
write.csv(table_results, "table_results.csv", row.names = FALSE)
```


### Question 9

Interpret the effect sizes (standardized estimates) for males and females. What are your conclusions?

### Question 10

Evaluate R-square for suicide risk for males and females. What do you find?


<details>
  <summary>Click for explanation</summary>

```{r}
summary(fit, rsquare = TRUE)
```
The R-square for suicide risk is .32 for females, and .15 for males. The model predicts suicide risk better for females.

\details

### Question 12

Calculate the total, direct and indirect effects (see practical week 5). The model we have made is a typical example of moderated mediation (i.e. the mediation effects are moderated by gender). In your own words, what are the differences in the mediation between males and females?

<details>
  <summary>Click for explanation</summary>
The total effects of depression and substance use on suicide risk are higher for females than males, but the total effect for selfesteem and hopelessness are very similar. 
\details

### Question 13

Compare your conclusion in the previous question with that of Mehta and colleagues (1998). Are your conclusions any different? Why?

<details>
  <summary>Click for explanation</summary>
Should be different: They do not test moderation explicitly, and report differences in all paths between males and females. In fact, the paths leading to suicide risk are different for males and females, but the mediation of depression through hopelessness is similar.
\details



<!--chapter:end:14-Week6_class.Rmd-->

# Week 7 - Putting it all together

**Path analysis on Theory of Reasoned Action data (file: toradata.sav)**

A popular theory in psychology to explain social behavior is the Theory of Reasoned Action (TORA) of Ajzen and Fishbein (sometimes called the Ajzen-Fishbein model). This states that behavior is predicted by behavioral intention, which is in turn predicted by the attitude toward the behavior and the subjective norm about the behavior. Ajzen and Fisbein originally proposed a complex way to measure attitude and norm, but in practice researchers usually measure the attitude by including a number of attitude questions, and the norm by including a number of questions about the perceived social norms (e.g., how many of your friends would do...). The intention is usually measured by a single question on a 7 or 9-point scale and the behavior is either a dichotomous outcome (yes/no) or a frequency (how often do you...). Later, a third determinant was added, perceived behavioral control. If people feel they have little control over their behavior, this will also influence their behavior.

### Loading the data

The TORA data are an artificial data set following results found by Reinecke (1998). The behavior under investigation is condom use by 16-24 year adolescents. The dependent variable ‘condom use’ is measured on a 5-point frequency scale (How often do you...), and the behavioral intention on a similar 5-point scale (In general do you intend to...). There are three attitude items about condom use (e.g., using a condom is awkward), three normative items (e.g., I think most of my friends would use...), and three control items (e.g., I know well how to use a condom), all measured on a 5-point Likert-type scale.

<details>
  <summary>Click for explanation</summary>
```{r}
library(foreign)
df <- read.spss("toradata.sav", use.value.labels = FALSE, to.data.frame = TRUE)
df$gender <- factor(df$gender, labels = c("woman", "man"))
```
\details


### Question 1

We have 3 indicators for attitude, norm and control, plus the variables intention and behavior. Carry out a factor analysis for the latent variables attitude, norm and control, and interpret the factor loadings of the indicators for each factor.

<!--*Hint: Use* `[]; %in%`-->

<details>
  <summary>Click for explanation</summary>
```{r}
library(lavaan)
TORA_cfa <- 'attit =~ attit1 + attit2 + attit3
             norm =~ norm1 + norm2 + norm3
             control =~ control1 + control2 + control3'

fit <- cfa(TORA_cfa, data = df)

summary(fit, fit.measures=TRUE)
```
\details

### Question 2

Set up a TORA model with attitude and norms as latent variables, both predicting behavorial intention and behavior. Analyze it. Interpret the model fit. If you find the model to fit, look at the results. How much variance does the model explain?

<details>
  <summary>Click for explanation</summary>
```{r}
TORA_sem <- 'attit =~ attit1 + attit2 + attit3
             norm =~ norm1 + norm2 + norm3
             
             intent ~ attit + norm
             behavior ~ intent'

fit <- sem(TORA_sem, data = df)

summary(fit, fit.measures = TRUE, rsquare = TRUE)
```
\details

### Question 3

Add control as a latent variable. Assume control has an effect on intention. Analyze this model and interpret the results. How much variance does the model explain?

<details>
  <summary>Click for explanation</summary>
  
The option `rsquare = TRUE` tells us how much variance is explained in each dependent variable:

```{r}
TORA_sem <- 'attit =~ attit1 + attit2 + attit3
             norm =~ norm1 + norm2 + norm3
             control =~ control1 + control2 + control3
             
             intent ~ attit + norm + control
             behavior ~ intent'

fit <- sem(TORA_sem, data = df)

summary(fit, fit.measures = TRUE, rsquare = TRUE)
```
\details

### Question 4

The TORA model forbids direct paths between attitude and norm and actual
behavior; the effect should be mediated totally by the behavioral intention. The effect of
control can be either direct or indirect; the TORA does not specify which. Test a model with both a direct and indirect effect of control on TORA and decide whether you want to keep or remove them. Decide which model you accept as the best model for these data, and explain how you decided to keep this model.

<details>
  <summary>Click for explanation</summary>
```{r}
TORA_dir <- 'attit =~ attit1 + attit2 + attit3
             norm =~ norm1 + norm2 + norm3
             control =~ control1 + control2 + control3
             
             intent ~ attit + norm + control
             behavior ~ intent'

TORA_ind <- 'attit =~ attit1 + attit2 + attit3
             norm =~ norm1 + norm2 + norm3
             control =~ control1 + control2 + control3
             
             intent ~ attit + norm + control
             behavior ~ intent + control'

fit_dir <- sem(TORA_dir, data = df)
fit_ind <- sem(TORA_ind, data = df)

```

To compare the models, we can use a $\chi^2$-difference test, using the `anova()` function or ``semTools::compareFits`:

```{r}
anova(fit_dir, fit_ind)
compareFit(Direct = fit_dir, Indirect = fit_ind)
```

\details

### Question 5

Use `semPlot::semPaths()` to plot both models, and check whether the picture corresponds with theory and with the way you intended to specify the model.

<details>
  <summary>Click for explanation</summary>
```{r}
library(semPlot)
semPaths(fit_dir, whatLabels = "est")
semPaths(fit_ind, whatLabels = "est")
```
\details

### Question 6

Before you learned about latent variables, you might have analyzed these data by computing sum or mean scores for attitude, norms, and control, and using these as observed variables in a path model. Set up a TORA model using the sum or mean scores of the variables, and compare the results with the previous analysis. Which model would you prefer, and why? 

*Hint: Use* `$; <-, [, rowMeans(), sum()`

<details>
  <summary>Click for explanation</summary>
```{r}
df$attit <- rowMeans(df[, c("attit1", "attit2", "attit3")])
# Or: df$norm <- rowSums(df[, c("attit1", "attit2", "attit3")])
df$norm <- rowMeans(df[, c("norm1", "norm2", "norm3")])
df$control <- rowMeans(df[, c("control1", "control2", "control3")])

TORA_obs <- 'intent ~ attit + norm + control
             behavior ~ intent'
fit_obs <- sem(TORA_obs, df)
```

The fit of this new model is worse than the model that included latent variables. However, we cannot directly compare fit indices, because these models are **not nested**! They are estimated on different variables. We will get a warning to emphasize this point. We can only interpret the Model Fit Indices table, and the Differences in Fit Indices, but not the Nested Model Comparison.

```{r}
compareFit(Latent = fit_dir,
           Observed = fit_obs)
```

We can also inspect the regression coefficients in both models:

```{r}
parameters_latent <- partable(fit_dir)
parameters_observed <- partable(fit_obs)
cbind(parameters_latent[parameters_latent$op == "~", c("lhs", "op", "rhs", "est")],
      est_obs = parameters_observed[parameters_observed$op == "~", c("est")])
```
\details

### Moderated mediation

We now want to investigate whether the Theory of Reasoned Action for condom use is different for boys and girls. This is a typical case of a moderated mediation model. 

### Question 7

Estimate the gender moderated model. What happened to the fit of the model when you compare it to your earlier model that didn’t include gender?

<details>
  <summary>Click for explanation</summary>
  
```{r}
fit_mod <- sem(TORA_sem, data = df, group = "gender")

summary(fit_mod, fit.measures = TRUE)
```
\details

### Measurement invariance

Before conducting this analysis, you should check for measurement invariance. There are different 'levels' of measurement invariance:

1. Configural: The same model in both groups
2. Metric: Same factor loadings
3. Scalar: Same factor loadings, and intercepts for the indicators.

We can specify these models with `groups.equal` constraints and then use `compareFit()`:

```{r}
library(semTools)
fit.config <- cfa(TORA_cfa, data = df, group = "gender")
fit.metric <- cfa(TORA_cfa, data = df, group = "gender",
                  group.equal = "loadings")
fit.scalar <- cfa(TORA_cfa, data = df, group = "gender",
                  group.equal = c("loadings", "intercepts"))
compareFit(Configural = fit.config, 
           Metric = fit.metric, 
           Scalar = fit.scalar)
```

You can see that imposing increasingly strict constraints does not significantly deteriorate the fit. So, scalar measurement invariance is supported.

### Question 8

Going back to the multi-group model, impose scalar invariance, and then determine whether it makes sense to constrain all the regression coefficients to be equal. Does it?

<details>
  <summary>Click for explanation</summary>
  
```{r}
fit_mod_invar <- sem(TORA_sem, data = df, 
                     group = "gender", 
                     group.equal = c("loadings", "intercepts"))
fit_mod_invar_reg <- sem(TORA_sem, data = df, 
                         group = "gender", 
                         group.equal = c("loadings", "intercepts", "regressions"))

compareFit(Multigroup = fit_mod_invar,
           Constrain_regressions = fit_mod_invar_reg)
```
\details


### Question 9

Depending on your answer to the previous question, investigate which regression coefficients of the TORA-model is the same or different for boys and girls. Decide on a final model and report how you choose your model.

With such a model, there are many ways to interpret the results. For example, you can look at the direct effects, indirect effects, explained variance, etc. Look at your final model and report on what you think are the most interesting results.
 

<!--chapter:end:15-Week7_home.Rmd-->

